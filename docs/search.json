[{},{"path":"index.html","id":"overview","chapter":"1 Overview","heading":"1 Overview","text":"sessions intended enable perform additional data analysis techniques appropriately confidently using R Python.Ongoing formative assessment exercisesOngoing formative assessment exercisesNo formal assessmentNo formal assessmentNo mathematical derivationsNo mathematical derivationsNo pen paper calculationsNo pen paper calculationsThey “mindlessly use stats program” course!","code":""},{"path":"index.html","id":"core-aims","chapter":"1 Overview","heading":"1.1 Core aims","text":"know presented non-standard dataset e.g.Know deal non-normal dataKnow analyse count dataBe able deal random effects","code":""},{"path":"index.html","id":"core-topics","chapter":"1 Overview","heading":"1.2 Core topics","text":"Generalised linear models","code":""},{"path":"index.html","id":"index-datasets","chapter":"1 Overview","heading":"1.3 Datasets","text":"course uses various data sets. easiest way accessing creating R-project RStudio. download data folder right-clicking link Save …. Next unzip file copy working directory. data accessible via <working-directory-name>/data..panelset{--panel-tab-font-family: inherit;}","code":""},{},{"path":"principal-component-analysis-pca.html","id":"principal-component-analysis-pca","chapter":"2 Principal component analysis (PCA)","heading":"2 Principal component analysis (PCA)","text":"","code":""},{"path":"principal-component-analysis-pca.html","id":"objectives","chapter":"2 Principal component analysis (PCA)","heading":"2.1 Objectives","text":"Understand PCAs can usefulBe able perform PCALearn plot interpret screeplotPlot interpret loadings PCA","code":""},{"path":"principal-component-analysis-pca.html","id":"purpose-and-aim","chapter":"2 Principal component analysis (PCA)","heading":"2.2 Purpose and aim","text":"statistical technique reducing dimensionality data set. technique aims find new set variables describing data. new variables made weighted sum old variables. weighting chosen new variables can ranked terms importance first new variable chosen account much variation data possible. second new variable chosen account much remaining variation data possible, many new variables old variables.","code":""},{"path":"principal-component-analysis-pca.html","id":"libraries-and-functions","chapter":"2 Principal component analysis (PCA)","heading":"2.3 Libraries and functions","text":"tidyverse","code":""},{"path":"principal-component-analysis-pca.html","id":"data","chapter":"2 Principal component analysis (PCA)","heading":"2.4 Data","text":"First need data! liven things bit, ’ll using data palmerpenguins package. package whole bunch data penguins. ’s love?Penguins\npenguins data set comes palmerpenguins package (information, see GitHub page).","code":""},{"path":"principal-component-analysis-pca.html","id":"visualise-the-data","chapter":"2 Principal component analysis (PCA)","heading":"2.5 Visualise the data","text":"First , let’s look data. always good idea get sense data.tidyverse\nFirst, load inspect data:can see different kinds variables, factors numerical. Also, appear missing data data set, probably deal .Lastly, careful year column: recognised numerical column (contains numbers), view factor, since years categorical meaning.get better sense data plot numerical variables , see possible correlation . However, quite , might easier just create correlation matrix.tidyverse\nFirst, load corrr package, allows us plot correlation matrix using tidyverse syntax:get message (error) correlation method used pearson, default. also get message missing values treated, complete pairwise comparisons made.can see , example, strong positive correlation flipper_length_mm body_mass_g. variable combinations seem reasonably well-correlated, flipper_length_mm bill_length_mm (positive) flipper_length_mm bill_depth_mm (negative).many different variables appear correlated , just lots variables data don’t know look, can useful reduce number variables. can dimension reduction methods, Principal Component Analysis (PCA) one.Basically, PCA replaces original variables new ones: principal components. principal components consist parts original variables.compare smoothy consisting , let’s say, 80% orange, 10% strawberry 10% banana (kale).Similarly, new principal component consist 80% flipper_length_mm, 10% body_mass_g 10% bill_depth_mm (still kale).","code":"\n# attach the data\ndata(package = 'palmerpenguins')\n\n# inspect the data\npenguins## # A tibble: 344 × 8\n##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n##    <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n##  1 Adelie  Torgersen           39.1          18.7               181        3750\n##  2 Adelie  Torgersen           39.5          17.4               186        3800\n##  3 Adelie  Torgersen           40.3          18                 195        3250\n##  4 Adelie  Torgersen           NA            NA                  NA          NA\n##  5 Adelie  Torgersen           36.7          19.3               193        3450\n##  6 Adelie  Torgersen           39.3          20.6               190        3650\n##  7 Adelie  Torgersen           38.9          17.8               181        3625\n##  8 Adelie  Torgersen           39.2          19.6               195        4675\n##  9 Adelie  Torgersen           34.1          18.1               193        3475\n## 10 Adelie  Torgersen           42            20.2               190        4250\n## # … with 334 more rows, and 2 more variables: sex <fct>, year <int>\nlibrary(corrr)\n\npenguins_corr <- penguins %>%\n  select(where(is.numeric)) %>%  # select the numerical columns\n  correlate() %>%                # calculate the correlations\n  rearrange()                    # arrange highly correlated variables together## \n## Correlation method: 'pearson'\n## Missing treated using: 'pairwise.complete.obs'\npenguins_corr## # A tibble: 5 × 6\n##   term         flipper_length_… body_mass_g bill_length_mm    year bill_depth_mm\n##   <chr>                   <dbl>       <dbl>          <dbl>   <dbl>         <dbl>\n## 1 flipper_len…           NA          0.871          0.656   0.170        -0.584 \n## 2 body_mass_g             0.871     NA              0.595   0.0422       -0.472 \n## 3 bill_length…            0.656      0.595         NA       0.0545       -0.235 \n## 4 year                    0.170      0.0422         0.0545 NA            -0.0604\n## 5 bill_depth_…           -0.584     -0.472         -0.235  -0.0604       NA"},{"path":"principal-component-analysis-pca.html","id":"performing-the-pca","chapter":"2 Principal component analysis (PCA)","heading":"2.6 Performing the PCA","text":"tidyverse\nperform PCA, ’ll use recipe() pre-processing steps:remove NA valuescentre predictorsscale predictors","code":"\npenguin_recipe <-\n  # take all variables\n  recipe(~ ., data = penguins) %>% \n  # specify the ID columns (non-numerical)\n  update_role(species, island, sex, year, new_role = \"id\") %>% \n  # remove missing values\n  step_naomit(all_predictors()) %>% \n  # scale the data\n  step_normalize(all_predictors()) %>%\n  # perform the PCA\n  step_pca(all_predictors(), id = \"pca\") %>% \n  # prepares the recipe by estimating the required parameters\n  prep()\n\npenguin_pca <- \n  penguin_recipe %>% \n  tidy(id = \"pca\") \n\npenguin_pca## # A tibble: 16 × 4\n##    terms                value component id   \n##    <chr>                <dbl> <chr>     <chr>\n##  1 bill_length_mm     0.455   PC1       pca  \n##  2 bill_depth_mm     -0.400   PC1       pca  \n##  3 flipper_length_mm  0.576   PC1       pca  \n##  4 body_mass_g        0.548   PC1       pca  \n##  5 bill_length_mm    -0.597   PC2       pca  \n##  6 bill_depth_mm     -0.798   PC2       pca  \n##  7 flipper_length_mm -0.00228 PC2       pca  \n##  8 body_mass_g       -0.0844  PC2       pca  \n##  9 bill_length_mm    -0.644   PC3       pca  \n## 10 bill_depth_mm      0.418   PC3       pca  \n## 11 flipper_length_mm  0.232   PC3       pca  \n## 12 body_mass_g        0.597   PC3       pca  \n## 13 bill_length_mm     0.146   PC4       pca  \n## 14 bill_depth_mm     -0.168   PC4       pca  \n## 15 flipper_length_mm -0.784   PC4       pca  \n## 16 body_mass_g        0.580   PC4       pca"},{"path":"principal-component-analysis-pca.html","id":"visualising-pcs","chapter":"2 Principal component analysis (PCA)","heading":"2.7 Visualising PCs","text":"Now ’ve performed PCA, can bit closer look. useful way looking much PCs (principal components) contributing amount variance explained create screeplot. Basically, plots percentage explained variance PC.tidyverse\ncan extract relevant data directly penguin_recipe object:definition, first principal component (PC1) always explain largest amount variation. case, PC1 explains almost 70% variance!’s pretty good going, since means instead look four variables, look just one still capture 70% variance data. number variables data set manageable, probably wouldn’t . However, data set hundreds variables, seeing can described well using PCs useful thing .","code":"\npenguin_recipe %>% \n  tidy(id = \"pca\", type = \"variance\") %>% \n  filter(terms == \"percent variance\") %>% \n  ggplot(aes(x = component, y = value)) + \n  geom_col() + \n  xlim(c(0, 5)) +\n  ylab(\"% of total variance\")"},{"path":"principal-component-analysis-pca.html","id":"loadings","chapter":"2 Principal component analysis (PCA)","heading":"2.8 Loadings","text":"Let’s think back smoothy metaphor. Remember smoothy made various fruits - just like PCs made parts original variables.Let’s, sake illustrating , assume following PC1:PC something called eigenvector, simplest terms line certain direction length.want calculate length eigenvector PC1, can employ Pythagoras (well, directly, just legacy). gives:\\(eigenvector \\, PC1 = \\sqrt{4^2 + 1^2} = 4.12\\)loading scores PC1 “parts” scaled length, .e.:can values plot loadings original variables. example, plotted PC1 PC2, wanted see much original variables contribute principal component, following:tidyverse\nloadings encoded value column pca object (penguin_pca). plot , need data “wide” format:can see four terms (original variables) four PCs. makes sense, maximum number principal components exceed number original variables.need bit data gymnastics, defining arrow style (vectors represented using arrows), plotting PCs, plotting loadings.Arrow:plot PC1 vs PC2, need extract relevant data. PCA-related data stored penguin_recipe object. Since people developed tidymodels quite fond verbs (’m really adjective kind--guy …) invented bake() function.logic : follow recipe() bake() . Yes, know.Anyways, use get data penguin_recipe, telling function shouldn’t expect new data (function also used model testing, data split training test data set).spits original table, PC values instead original variables. can use information plot PCs, focussing PC1 vs PC2:Lastly (finally), take plot add loading data top :Slightly annoyingly, can cause overlap labels. Also, many different variables , ’ll become rather tricky interpret web vectors.things look pretty OK still. One thing immediately obvious data Gentoo penguins PCs quite distinct Adelie Chinstrap penguins.PC-perspective, flipper_length_mm body_mass_g variables make PC1, since vectors almost horizontal.contrast, bill_length_mm bill_depth_mm contribute lot PC2. also contribute positively (bill_length_mm) negatively (bill_depth_mm) PC1.mentioned, vectors can become bit messy terms visualisation. can useful represent loadings differently. Note , point, ’m aware method allows reasonably straightforward anything R’s tidyverse.tidyverse\nfollowing work, need load tidytext libary. can follows:gives us loadings variable, facetted principal component. reason ’re plotting absolute values, can compare positive negative contributions. PC absolute loadings sorted descending order. distinguish positive negative loadings use colours.important keep amount variance explained PC mind. example, PC3 explains around 9% variance. although bill length body mass contribute substantially PC3, contribution PC3 remains small.","code":"\n# get pca loadings into wider format\npca_wider <- penguin_pca %>% \n  pivot_wider(names_from = component, id_cols = terms)\n\npca_wider## # A tibble: 4 × 5\n##   terms                PC1      PC2    PC3    PC4\n##   <chr>              <dbl>    <dbl>  <dbl>  <dbl>\n## 1 bill_length_mm     0.455 -0.597   -0.644  0.146\n## 2 bill_depth_mm     -0.400 -0.798    0.418 -0.168\n## 3 flipper_length_mm  0.576 -0.00228  0.232 -0.784\n## 4 body_mass_g        0.548 -0.0844   0.597  0.580\n# define arrow style\narrow_style <- arrow(length = unit(2, \"mm\"),\n                     type = \"closed\")\nbake(penguin_recipe, new_data = NULL)## # A tibble: 342 × 8\n##    species island    sex     year    PC1      PC2     PC3     PC4\n##    <fct>   <fct>     <fct>  <int>  <dbl>    <dbl>   <dbl>   <dbl>\n##  1 Adelie  Torgersen male    2007 -1.84  -0.0476   0.232   0.523 \n##  2 Adelie  Torgersen female  2007 -1.30   0.428    0.0295  0.402 \n##  3 Adelie  Torgersen female  2007 -1.37   0.154   -0.198  -0.527 \n##  4 Adelie  Torgersen female  2007 -1.88   0.00205  0.618  -0.478 \n##  5 Adelie  Torgersen male    2007 -1.91  -0.828    0.686  -0.207 \n##  6 Adelie  Torgersen female  2007 -1.76   0.351   -0.0276  0.504 \n##  7 Adelie  Torgersen male    2007 -0.809 -0.522    1.33    0.338 \n##  8 Adelie  Torgersen <NA>    2007 -1.83   0.769    0.689  -0.427 \n##  9 Adelie  Torgersen <NA>    2007 -1.19  -1.02     0.729   0.333 \n## 10 Adelie  Torgersen <NA>    2007 -1.73   0.787   -0.205   0.0205\n## # … with 332 more rows\npca_plot <-\n  bake(penguin_recipe, new_data = NULL) %>%\n  ggplot(aes(PC1, PC2)) +\n  geom_point(aes(colour = species), # colour the data\n             alpha = 0.8,           # add transparency\n             size = 2)              # make the data points bigger\n\npca_plot\npca_plot +\n  # define the vector\n  geom_segment(data = pca_wider,\n               aes(xend = PC1, yend = PC2), \n               x = 0, \n               y = 0, \n               arrow = arrow_style) + \n  # add the text labels\n  geom_text(data = pca_wider,\n            aes(x = PC1, y = PC2, label = terms), \n            hjust = 0, \n            vjust = 1,\n            size = 5) \nlibrary(tidytext)\npenguin_pca %>%\n  mutate(terms = reorder_within(terms, \n                                abs(value), \n                                component)) %>%\n  ggplot(aes(abs(value), terms, fill = value > 0)) +\n  geom_col() +\n  facet_wrap(~ component, scales = \"free_y\") +\n  tidytext::scale_y_reordered() +\n  labs(\n    x = \"Absolute value of contribution\",\n    y = NULL, fill = \"Positive?\"\n  ) "},{"path":"principal-component-analysis-pca.html","id":"exercise-heptathlon","chapter":"2 Principal component analysis (PCA)","heading":"2.9 Exercise: Heptathlon","text":"Exercise 2.1  First , heptathlon actual word. Seven sports events one go! old data set keeps track scores/times 25 athletes heptathlon event. can’t remember data collected, main reason ’ve kept show fleeting concept ‘country’ . good proportion countries longer exist…Let’s get philosophical see can data set. like following:load datacreate correlation matrix visualise highly correlated pairperform PCAcreate screeplot see many PCs bestcalculate loadings PC1 PC2 visualise themtidyverseload dataNote many country codes changed (URS, GDR, FRG!). Also, humbug, HOL ‘Holland’ country, Netherlands !. search see world constantly changing…create correlation matrix visualise highly correlated pairLooking correlation matrix, highly correlated pair longjump hurdles (-0.91). () programmatically, ’s small matrix life short.seems better long jump, faster (thus better) hurdles. guess makes sense interesting data leg length see correlation …perform PCAcreate screeplot see many PCs bestIt’s clear PC1 explains heck lot variance data (around 65%). PC2 explains bit , just 20%. two PCs combined explain 85% variance data, pretty good.Things get bit less clear go “” PCs: PC3 PC4 explain roughly amount variance, ’d include PC3 also include PC4. However, since explain around 8% variance contribute much, leave PC1 PC2.calculate loadings PC1 PC2 visualise themWith 7 original variables, plotting PC1 PC2 loadings bit unclear. case ’ll just create bar chart PC absolute contributions original variable.Just illustrate much informative drawing vectors:Told ya.Well, can conclude ? First , ’s worth noting 25 observations 7 variables, limits analysis bit.can see PC1 (particularly combined PC2) able explain quite big chunk variance data. However, keep mind observation individual athlete. ideally want plot names athletes PC1 vs PC2 plot see individual performances 7 events compare.","code":"\nhept <- read_csv(\"data/heptathlon.csv\")\n\nhept## # A tibble: 25 × 8\n##    athlete             hurdles highjump  shot run200m longjump javelin run800m\n##    <chr>                 <dbl>    <dbl> <dbl>   <dbl>    <dbl>   <dbl>   <dbl>\n##  1 Joyner-Kersee (USA)    12.7     1.86  15.8    22.6     7.27    45.7    129.\n##  2 John (GDR)             12.8     1.8   16.2    23.6     6.71    42.6    126.\n##  3 Behmer (GDR)           13.2     1.83  14.2    23.1     6.68    44.5    124.\n##  4 Sablovskaite (URS)     13.6     1.8   15.2    23.9     6.25    42.8    132.\n##  5 Choubenkova (URS)      13.5     1.74  14.8    23.9     6.32    47.5    128.\n##  6 Schulz (GDR)           13.8     1.83  13.5    24.6     6.33    42.8    126.\n##  7 Fleming (AUS)          13.4     1.8   12.9    23.6     6.37    40.3    133.\n##  8 Greiner (USA)          13.6     1.8   14.1    24.5     6.47    38      134.\n##  9 Lajbnerova (CZE)       13.6     1.83  14.3    24.9     6.11    42.2    136.\n## 10 Bouraga (URS)          13.2     1.77  12.6    23.6     6.28    39.1    135.\n## # … with 15 more rows\nhept_corr <- hept %>%\n  select(where(is.numeric)) %>%  # select the numerical columns\n  correlate() %>%                # calculate the correlations\n  rearrange()                    # arrange highly correlated variables together## \n## Correlation method: 'pearson'\n## Missing treated using: 'pairwise.complete.obs'\nhept_corr## # A tibble: 7 × 8\n##   term      hurdles run200m run800m  javelin   shot highjump longjump\n##   <chr>       <dbl>   <dbl>   <dbl>    <dbl>  <dbl>    <dbl>    <dbl>\n## 1 hurdles  NA         0.774  0.779  -0.00776 -0.651 -0.811    -0.912 \n## 2 run200m   0.774    NA      0.617  -0.333   -0.683 -0.488    -0.817 \n## 3 run800m   0.779     0.617 NA       0.0200  -0.420 -0.591    -0.700 \n## 4 javelin  -0.00776  -0.333  0.0200 NA        0.269  0.00215   0.0671\n## 5 shot     -0.651    -0.683 -0.420   0.269   NA      0.441     0.743 \n## 6 highjump -0.811    -0.488 -0.591   0.00215  0.441 NA         0.782 \n## 7 longjump -0.912    -0.817 -0.700   0.0671   0.743  0.782    NA\nggplot(data = hept,\n       aes(x = longjump,\n           y = hurdles)) +\n  geom_point() +\n  labs(title = \"longjump vs hurdles\")\nhept_recipe <-\n  # take all variables\n  recipe(~ ., data = hept) %>% \n  # specify the ID columns (non-numerical)\n  update_role(athlete, new_role = \"id\") %>% \n  # remove missing values\n  step_naomit(all_predictors()) %>% \n  # scale the data\n  step_normalize(all_predictors()) %>%\n  # perform the PCA\n  step_pca(all_predictors(), id = \"pca\") %>% \n  # prepares the recipe by estimating the required parameters\n  prep()\n\nhept_pca <- \n  hept_recipe %>% \n  tidy(id = \"pca\") \n\nhept_pca## # A tibble: 49 × 4\n##    terms      value component id   \n##    <chr>      <dbl> <chr>     <chr>\n##  1 hurdles   0.453  PC1       pca  \n##  2 highjump -0.377  PC1       pca  \n##  3 shot     -0.363  PC1       pca  \n##  4 run200m   0.408  PC1       pca  \n##  5 longjump -0.456  PC1       pca  \n##  6 javelin  -0.0754 PC1       pca  \n##  7 run800m   0.375  PC1       pca  \n##  8 hurdles  -0.158  PC2       pca  \n##  9 highjump  0.248  PC2       pca  \n## 10 shot     -0.289  PC2       pca  \n## # … with 39 more rows\nhept_recipe %>% \n  tidy(id = \"pca\", type = \"variance\") %>% \n  filter(terms == \"percent variance\") %>% \n  ggplot(aes(x = component, y = value)) + \n  geom_col() + \n  ylab(\"% of total variance\")\nhept_pca %>%\n  filter(component %in% c(\"PC1\", \"PC2\")) %>% \n  mutate(terms = reorder_within(terms, \n                                abs(value), \n                                component)) %>%\n  ggplot(aes(abs(value), terms, fill = value > 0)) +\n  geom_col() +\n  facet_wrap(~ component, scales = \"free_y\") +\n  tidytext::scale_y_reordered() +\n  labs(\n    x = \"Absolute value of contribution\",\n    y = NULL, fill = \"Positive?\"\n  ) \n# get pca loadings into wider format\npca_wider <- hept_pca %>% \n  pivot_wider(names_from = component, id_cols = terms)\n\npca_plot <-\n  bake(hept_recipe, new_data = NULL) %>%\n  ggplot(aes(PC1, PC2)) +\n  geom_point(alpha = 0.8, # add transparency\n             size = 2)    # make the data points bigger\n\n# define arrow style\narrow_style <- arrow(length = unit(2, \"mm\"),\n                     type = \"closed\")\n\npca_plot +\n  # define the vector\n  geom_segment(data = pca_wider,\n               aes(xend = PC1, yend = PC2), \n               x = 0, \n               y = 0, \n               arrow = arrow_style,\n               colour = \"red\") + \n  # add the text labels\n  geom_text(data = pca_wider,\n            aes(x = PC1, y = PC2, label = terms), \n            hjust = 0, \n            vjust = 1,\n            size = 5) "},{"path":"principal-component-analysis-pca.html","id":"load-the-data","chapter":"2 Principal component analysis (PCA)","heading":"2.9.1 Load the data","text":"load dataNote many country codes changed (URS, GDR, FRG!). Also, humbug, HOL ‘Holland’ country, Netherlands !. search see world constantly changing…","code":"\nhept <- read_csv(\"data/heptathlon.csv\")\n\nhept## # A tibble: 25 × 8\n##    athlete             hurdles highjump  shot run200m longjump javelin run800m\n##    <chr>                 <dbl>    <dbl> <dbl>   <dbl>    <dbl>   <dbl>   <dbl>\n##  1 Joyner-Kersee (USA)    12.7     1.86  15.8    22.6     7.27    45.7    129.\n##  2 John (GDR)             12.8     1.8   16.2    23.6     6.71    42.6    126.\n##  3 Behmer (GDR)           13.2     1.83  14.2    23.1     6.68    44.5    124.\n##  4 Sablovskaite (URS)     13.6     1.8   15.2    23.9     6.25    42.8    132.\n##  5 Choubenkova (URS)      13.5     1.74  14.8    23.9     6.32    47.5    128.\n##  6 Schulz (GDR)           13.8     1.83  13.5    24.6     6.33    42.8    126.\n##  7 Fleming (AUS)          13.4     1.8   12.9    23.6     6.37    40.3    133.\n##  8 Greiner (USA)          13.6     1.8   14.1    24.5     6.47    38      134.\n##  9 Lajbnerova (CZE)       13.6     1.83  14.3    24.9     6.11    42.2    136.\n## 10 Bouraga (URS)          13.2     1.77  12.6    23.6     6.28    39.1    135.\n## # … with 15 more rows"},{"path":"principal-component-analysis-pca.html","id":"correlations","chapter":"2 Principal component analysis (PCA)","heading":"2.9.2 Correlations","text":"create correlation matrix visualise highly correlated pairLooking correlation matrix, highly correlated pair longjump hurdles (-0.91). () programmatically, ’s small matrix life short.seems better long jump, faster (thus better) hurdles. guess makes sense interesting data leg length see correlation …","code":"\nhept_corr <- hept %>%\n  select(where(is.numeric)) %>%  # select the numerical columns\n  correlate() %>%                # calculate the correlations\n  rearrange()                    # arrange highly correlated variables together## \n## Correlation method: 'pearson'\n## Missing treated using: 'pairwise.complete.obs'\nhept_corr## # A tibble: 7 × 8\n##   term      hurdles run200m run800m  javelin   shot highjump longjump\n##   <chr>       <dbl>   <dbl>   <dbl>    <dbl>  <dbl>    <dbl>    <dbl>\n## 1 hurdles  NA         0.774  0.779  -0.00776 -0.651 -0.811    -0.912 \n## 2 run200m   0.774    NA      0.617  -0.333   -0.683 -0.488    -0.817 \n## 3 run800m   0.779     0.617 NA       0.0200  -0.420 -0.591    -0.700 \n## 4 javelin  -0.00776  -0.333  0.0200 NA        0.269  0.00215   0.0671\n## 5 shot     -0.651    -0.683 -0.420   0.269   NA      0.441     0.743 \n## 6 highjump -0.811    -0.488 -0.591   0.00215  0.441 NA         0.782 \n## 7 longjump -0.912    -0.817 -0.700   0.0671   0.743  0.782    NA\nggplot(data = hept,\n       aes(x = longjump,\n           y = hurdles)) +\n  geom_point() +\n  labs(title = \"longjump vs hurdles\")"},{"path":"principal-component-analysis-pca.html","id":"pca","chapter":"2 Principal component analysis (PCA)","heading":"2.9.3 PCA","text":"perform PCA","code":"\nhept_recipe <-\n  # take all variables\n  recipe(~ ., data = hept) %>% \n  # specify the ID columns (non-numerical)\n  update_role(athlete, new_role = \"id\") %>% \n  # remove missing values\n  step_naomit(all_predictors()) %>% \n  # scale the data\n  step_normalize(all_predictors()) %>%\n  # perform the PCA\n  step_pca(all_predictors(), id = \"pca\") %>% \n  # prepares the recipe by estimating the required parameters\n  prep()\n\nhept_pca <- \n  hept_recipe %>% \n  tidy(id = \"pca\") \n\nhept_pca## # A tibble: 49 × 4\n##    terms      value component id   \n##    <chr>      <dbl> <chr>     <chr>\n##  1 hurdles   0.453  PC1       pca  \n##  2 highjump -0.377  PC1       pca  \n##  3 shot     -0.363  PC1       pca  \n##  4 run200m   0.408  PC1       pca  \n##  5 longjump -0.456  PC1       pca  \n##  6 javelin  -0.0754 PC1       pca  \n##  7 run800m   0.375  PC1       pca  \n##  8 hurdles  -0.158  PC2       pca  \n##  9 highjump  0.248  PC2       pca  \n## 10 shot     -0.289  PC2       pca  \n## # … with 39 more rows"},{"path":"principal-component-analysis-pca.html","id":"screeplot","chapter":"2 Principal component analysis (PCA)","heading":"2.9.4 Screeplot","text":"create screeplot see many PCs bestIt’s clear PC1 explains heck lot variance data (around 65%). PC2 explains bit , just 20%. two PCs combined explain 85% variance data, pretty good.Things get bit less clear go “” PCs: PC3 PC4 explain roughly amount variance, ’d include PC3 also include PC4. However, since explain around 8% variance contribute much, leave PC1 PC2.","code":"\nhept_recipe %>% \n  tidy(id = \"pca\", type = \"variance\") %>% \n  filter(terms == \"percent variance\") %>% \n  ggplot(aes(x = component, y = value)) + \n  geom_col() + \n  ylab(\"% of total variance\")"},{"path":"principal-component-analysis-pca.html","id":"loadings-1","chapter":"2 Principal component analysis (PCA)","heading":"2.9.5 Loadings","text":"calculate loadings PC1 PC2 visualise themWith 7 original variables, plotting PC1 PC2 loadings bit unclear. case ’ll just create bar chart PC absolute contributions original variable.Just illustrate much informative drawing vectors:Told ya.","code":"\nhept_pca %>%\n  filter(component %in% c(\"PC1\", \"PC2\")) %>% \n  mutate(terms = reorder_within(terms, \n                                abs(value), \n                                component)) %>%\n  ggplot(aes(abs(value), terms, fill = value > 0)) +\n  geom_col() +\n  facet_wrap(~ component, scales = \"free_y\") +\n  tidytext::scale_y_reordered() +\n  labs(\n    x = \"Absolute value of contribution\",\n    y = NULL, fill = \"Positive?\"\n  ) \n# get pca loadings into wider format\npca_wider <- hept_pca %>% \n  pivot_wider(names_from = component, id_cols = terms)\n\npca_plot <-\n  bake(hept_recipe, new_data = NULL) %>%\n  ggplot(aes(PC1, PC2)) +\n  geom_point(alpha = 0.8, # add transparency\n             size = 2)    # make the data points bigger\n\n# define arrow style\narrow_style <- arrow(length = unit(2, \"mm\"),\n                     type = \"closed\")\n\npca_plot +\n  # define the vector\n  geom_segment(data = pca_wider,\n               aes(xend = PC1, yend = PC2), \n               x = 0, \n               y = 0, \n               arrow = arrow_style,\n               colour = \"red\") + \n  # add the text labels\n  geom_text(data = pca_wider,\n            aes(x = PC1, y = PC2, label = terms), \n            hjust = 0, \n            vjust = 1,\n            size = 5) "},{"path":"principal-component-analysis-pca.html","id":"conclusion","chapter":"2 Principal component analysis (PCA)","heading":"2.9.6 Conclusion","text":"Well, can conclude ? First , ’s worth noting 25 observations 7 variables, limits analysis bit.can see PC1 (particularly combined PC2) able explain quite big chunk variance data. However, keep mind observation individual athlete. ideally want plot names athletes PC1 vs PC2 plot see individual performances 7 events compare.","code":""},{"path":"principal-component-analysis-pca.html","id":"key-points","chapter":"2 Principal component analysis (PCA)","heading":"2.10 Key points","text":"PCA allows reduce large number variables fewer principal componentsEach PC made combination original variables captures much variance within data possibleThe loadings tell much original variable contributes PCA screeplot graphical representation amount variance explained PC","code":""}]
