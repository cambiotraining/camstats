[{},{"path":"index.html","id":"overview","chapter":"1 Overview","heading":"1 Overview","text":"sessions intended enable perform additional data analysis techniques appropriately confidently using R Python.Ongoing formative assessment exercisesOngoing formative assessment exercisesNo formal assessmentNo formal assessmentNo mathematical derivationsNo mathematical derivationsNo pen paper calculationsNo pen paper calculationsThey “mindlessly use stats program” course!","code":""},{"path":"index.html","id":"core-aims","chapter":"1 Overview","heading":"1.1 Core aims","text":"know presented non-standard dataset e.g.Know deal non-normal dataKnow analyse count dataBe able deal random effects","code":""},{"path":"index.html","id":"core-topics","chapter":"1 Overview","heading":"1.2 Core topics","text":"Generalised linear models","code":""},{"path":"index.html","id":"index-datasets","chapter":"1 Overview","heading":"1.3 Datasets","text":"course uses various data sets. easiest way accessing creating R-project RStudio. download data folder right-clicking link Save …. Next unzip file copy working directory. data accessible via <working-directory-name>/data..panelset{--panel-tab-font-family: inherit;}","code":"## Registered S3 method overwritten by 'tune':\n##   method                   from   \n##   required_pkgs.model_spec parsnip## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.4 ──## ✓ dials        0.1.0     ✓ rsample      0.1.1\n## ✓ infer        1.0.0     ✓ tune         0.1.6\n## ✓ modeldata    0.1.1     ✓ workflows    0.2.4\n## ✓ parsnip      0.1.7     ✓ workflowsets 0.1.0\n## ✓ recipes      0.2.0     ✓ yardstick    0.0.9## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n## x scales::discard() masks purrr::discard()\n## x dplyr::filter()   masks stats::filter()\n## x recipes::fixed()  masks stringr::fixed()\n## x dplyr::lag()      masks stats::lag()\n## x yardstick::spec() masks readr::spec()\n## x recipes::step()   masks stats::step()\n## • Use tidymodels_prefer() to resolve common conflicts.## Warning: 'xaringanExtra::style_panelset' is deprecated.\n## Use 'style_panelset_tabs' instead.\n## See help(\"Deprecated\")"},{},{"path":"logistic-models-binary-response.html","id":"logistic-models-binary-response","chapter":"2 Logistic Models – Binary Response","heading":"2 Logistic Models – Binary Response","text":"","code":""},{"path":"logistic-models-binary-response.html","id":"objectives","chapter":"2 Logistic Models – Binary Response","heading":"2.1 Objectives","text":"QuestionsObjectives","code":""},{"path":"logistic-models-binary-response.html","id":"libraries-and-functions","chapter":"2 Logistic Models – Binary Response","heading":"2.2 Libraries and functions","text":"tidyverse","code":""},{"path":"logistic-models-binary-response.html","id":"datasets","chapter":"2 Logistic Models – Binary Response","heading":"2.3 Datasets","text":"DiabetesThe example section uses following data set:data/diabetes.csvThis data set comprising 768 observations three variables (one dependent two predictor variables). records results diabetes test_result binary variable (1 positive result, 0 negative result), along result glucose test diastolic blood pressure 767 women. variables called test_result, glucose diastolic.","code":""},{"path":"logistic-models-binary-response.html","id":"visualise-the-data","chapter":"2 Logistic Models – Binary Response","heading":"2.4 Visualise the data","text":"First load data, visualise . needed, load tidyverse package using:tidyverse\nFirst, load inspect data:Looking data, can see test_result column contains zeros ones. test result outcomes actually numeric representations.cause problems later, need tell R see values factors.can plot data:looks though variable glucose may effect results diabetes test since positive test results seem slightly higher negative test results.can visualise differently plotting data points classic binary response plot:","code":"\ndiabetes <- read_csv(\"data/diabetes.csv\")\ndiabetes <- diabetes %>% \n  mutate(test_result = as_factor(test_result))\ndiabetes %>% \n  ggplot(aes(x = test_result, y = glucose)) +\n  geom_boxplot()\ndiabetes %>% \n  ggplot(aes(x = glucose, y = test_result)) +\n  geom_point()"},{"path":"logistic-models-binary-response.html","id":"construct-the-model","chapter":"2 Logistic Models – Binary Response","heading":"2.5 Construct the model","text":"different ways construct logistic model either R Python.tidyverseIn tidyverse access useful package: parsnip, part tidymodels package. advantage using parsnip code syntax stay different kind model comparisons. , learning curve might bit steeper start , pay dividend long-term (just like started using R!).First, need load tidymodels (install first, needed):Next, can create model:summarise output model, get following information:look informative. get actually list objects contain kinds information.can get insight model parameters following:base R\nbase R use glm() function, works similar way lm() function.define model follows:forget include family argument glm function just performs ordinary linear model fit (lm function)Next, can summarise model :’s lot unpack take deep breath (make sure coffee) continuing…first lines just confirm model ’ve fitting (trust , can useful ’re middle load analysis ’ve lost track hell going !)first lines just confirm model ’ve fitting (trust , can useful ’re middle load analysis ’ve lost track hell going !)next block called Deviance Residuals. isn’t particularly useful, just know: linear models residuals calculated data point squared added get SS (sum squares), used fit model. generalised linear models don’t use SS fit model instead use entirely different method called maximum likelihood. fitting procedure generates different quantity, called Deviance, analogue SS. deviance zero indicates best model hope bigger values indicate model doesn’t fit quite well. deviance residuals values associated data point, squared summed give deviance model (exact analogy normal residuals). ’re unlikely ever need know , time hands decided share little nugget 😉.next block called Deviance Residuals. isn’t particularly useful, just know: linear models residuals calculated data point squared added get SS (sum squares), used fit model. generalised linear models don’t use SS fit model instead use entirely different method called maximum likelihood. fitting procedure generates different quantity, called Deviance, analogue SS. deviance zero indicates best model hope bigger values indicate model doesn’t fit quite well. deviance residuals values associated data point, squared summed give deviance model (exact analogy normal residuals). ’re unlikely ever need know , time hands decided share little nugget 😉.Coefficients block next. main numbers extract output two numbers underneath Estimate.Std: (Intercept) -5.611732 glucose 0.039510.coefficients logistic model equation need placed correct equation want able calculate probability positive diabetes test result given glucose level.Coefficients block next. main numbers extract output two numbers underneath Estimate.Std: (Intercept) -5.611732 glucose 0.039510.coefficients logistic model equation need placed correct equation want able calculate probability positive diabetes test result given glucose level.\\[\\begin{equation}\nP(positive \\ test_result) = \\frac{1}{1 + {e}^{-(-5.61 +  0.040 \\cdot glucose)}}\n\\end{equation}\\]p values (Pr(>|z|) end coefficient row merely show whether particular coefficient significantly different zero. similar p-values obtained summary output linear model, , continuous predictors p-values can used rough guide whether predictor important (case glucose appears significant). However, p-values aren’t great multiple predictor variables, categorical predictors multiple levels (since output give us p-value level rather predictor whole).p values (Pr(>|z|) end coefficient row merely show whether particular coefficient significantly different zero. similar p-values obtained summary output linear model, , continuous predictors p-values can used rough guide whether predictor important (case glucose appears significant). However, p-values aren’t great multiple predictor variables, categorical predictors multiple levels (since output give us p-value level rather predictor whole).next line tells us dispersion parameter assumed 1 binomial model. Dispersion property says whether data less spread around logistic curve expect. dispersion parameter 1 says data spread exactly expect. Greater 1 called -dispersion less 1 called -dispersion. line saying fitted model, assuming dispersion data exactly 1. binary data, like , data - -dispersed something ’ll need check sorts generalised linear models.next line tells us dispersion parameter assumed 1 binomial model. Dispersion property says whether data less spread around logistic curve expect. dispersion parameter 1 says data spread exactly expect. Greater 1 called -dispersion less 1 called -dispersion. line saying fitted model, assuming dispersion data exactly 1. binary data, like , data - -dispersed something ’ll need check sorts generalised linear models.last three lines relate quantities called deviance AIC (Akaike Information Criterion).\nsaid just , deviance values equivalent Sums Squares values linear models (product technique used fit curve data). can used metric goodness fit model, deviance 0 indicating perfect fitting model. deviance null model (.e. model without predictors, basically saying probability getting positive diabetes score constant doesn’t depend glucose level) given first line deviance actual model given residual deviance line. see can use deviance two things.\ncheck whether model actually good (.e. way look like ’s close data). akin R2 values linear models.\ncheck model ’ve specified better null model.\n’s important realise two things can independent ; can model significantly better null model whilst still rubbish overall (null model even rubbish comparison), can model brilliant yet still better null model (case null model already brilliant).\n\nlast three lines relate quantities called deviance AIC (Akaike Information Criterion).said just , deviance values equivalent Sums Squares values linear models (product technique used fit curve data). can used metric goodness fit model, deviance 0 indicating perfect fitting model. deviance null model (.e. model without predictors, basically saying probability getting positive diabetes score constant doesn’t depend glucose level) given first line deviance actual model given residual deviance line. see can use deviance two things.\ncheck whether model actually good (.e. way look like ’s close data). akin R2 values linear models.\ncheck model ’ve specified better null model.\n’s important realise two things can independent ; can model significantly better null model whilst still rubbish overall (null model even rubbish comparison), can model brilliant yet still better null model (case null model already brilliant).\ncheck whether model actually good (.e. way look like ’s close data). akin R2 values linear models.check model ’ve specified better null model.\n’s important realise two things can independent ; can model significantly better null model whilst still rubbish overall (null model even rubbish comparison), can model brilliant yet still better null model (case null model already brilliant).found previous practical AIC value meaningless , allow us compare model another model different terms (model smaller AIC value better fitting model).found previous practical AIC value meaningless , allow us compare model another model different terms (model smaller AIC value better fitting model).","code":"\n# install.packages(\"tidymodels\")\nlibrary(tidymodels)\nglm_diabetes <- logistic_reg() %>% \n  set_engine(\"glm\") %>% \n  fit(test_result ~ glucose, data = diabetes)\nsummary(glm_diabetes)##         Length Class        Mode     \n## lvl      2     -none-       character\n## spec     6     logistic_reg list     \n## fit     30     glm          list     \n## preproc  1     -none-       list     \n## elapsed  5     proc_time    numeric\nglm_diabetes %>%\n  extract_fit_engine() %>% \n  glance()## # A tibble: 1 × 8\n##   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n##           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n## 1          937.     727  -376.  756.  765.     752.         726   728\nglm_diabetes_r <- glm(test_result ~ glucose,\n                      data = diabetes,\n                      family = binomial)\nsummary(glm_diabetes_r)## \n## Call:\n## glm(formula = test_result ~ glucose, family = binomial, data = diabetes)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.1353  -0.7819  -0.5189   0.8269   2.2832  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -5.611732   0.442289  -12.69   <2e-16 ***\n## glucose      0.039510   0.003398   11.63   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 936.6  on 727  degrees of freedom\n## Residual deviance: 752.2  on 726  degrees of freedom\n## AIC: 756.2\n## \n## Number of Fisher Scoring iterations: 4"},{"path":"logistic-models-binary-response.html","id":"using-the-model-to-make-predictions","chapter":"2 Logistic Models – Binary Response","heading":"2.6 Using the model to make predictions","text":"got new glucose level data wanted predict people might diabetes ?use existing model feed data:tidyverseAlthough able get predicted outcomes, like stress point running model. important realise model (statistical models) creates predicted outcome based certain probabilities. therefore much informative look probable predicted outcomes . can follows:see predicted outcomes, encoded .pred_class remain , now two new columns present: .pred_0 .pred_1. give probability outcome 0 1. first value means 14% chance diabetes test return negative result around 86% chance return positive result.","code":"\n# create a dummy data set using some hypothetical glucose measurements\ndiabetes_newdata <- tibble(glucose = c(188, 122, 83, 76, 144))\n\n# predict if the patients have diabetes or not\npredict(glm_diabetes, new_data = diabetes_newdata)## # A tibble: 5 × 1\n##   .pred_class\n##   <fct>      \n## 1 1          \n## 2 0          \n## 3 0          \n## 4 0          \n## 5 1\ndiabetes_newdata %>% \n  select(glucose) %>% \n  bind_cols(predict(glm_diabetes, diabetes_newdata)) %>% \n  # add the probabilities for both outcomes\n  bind_cols(predict(glm_diabetes, diabetes_newdata, type = \"prob\")) ## # A tibble: 5 × 4\n##   glucose .pred_class .pred_0 .pred_1\n##     <dbl> <fct>         <dbl>   <dbl>\n## 1     188 1             0.140  0.860 \n## 2     122 0             0.688  0.312 \n## 3      83 0             0.912  0.0885\n## 4      76 0             0.931  0.0686\n## 5     144 1             0.481  0.519"},{"path":"logistic-models-binary-response.html","id":"model-evaluation","chapter":"2 Logistic Models – Binary Response","heading":"2.7 Model evaluation","text":"far ’ve constructed logistic model fed new data make predictions possible outcome diabetes test, depending glucose level given patient. gave us diabetes test predictions , importantly, probabilities whether test come back negative (0) positive (1).question ’d like ask point: reliable model?explore , need take step back.tidyverseWhen created model, used data. However, good way assessing model fit actually split data two:training data set use fit modela test data set validate model measure model performanceBefore split data, let’s closer look data set. count many diabetes test results negative (0) positive (1), see counts evenly split.can consequences start splitting data training test set. splitting data two parts - data goes training set - data left afterwards can use test good predictions model . However, need make sure proportion negative positive diabetes test outcomes remains roughly .rsample package couple useful functions allow us just can use strata argument keep proportions less .can check initial_split() function done:Although seems bit overkill, now single function can can use prepare recipe train model resulting predictors:creates object called diabetes_fit, contains final recipe fitted model objects. can extract model recipe objects several helper functions:far, done following:Built model (diabetes_mod),Created pre-processing recipe (diabetes_rec),Combined model recipe workflow (diabetes_wflow)Trained workflow using fit() function (diabetes_fit)results generated differ much values obtained entire data set. However, based 3/4 data (training data set). , still test data set available apply workflow data model yet seen.can now evaluate model. One way using area ROC curve metric.","code":"\ndiabetes %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))## # A tibble: 2 × 3\n##   test_result     n  prop\n##   <fct>       <int> <dbl>\n## 1 0             478 0.657\n## 2 1             250 0.343\n# Use 75% of the data to create the training data set\ndata_split <- initial_split(diabetes, strata = test_result)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n# proportion of data allocated to the training set\nnrow(train_data) / nrow(diabetes)## [1] 0.7486264\n# proportion of diabetes test results for the training and test data sets\ntrain_data %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))## # A tibble: 2 × 3\n##   test_result     n  prop\n##   <fct>       <int> <dbl>\n## 1 0             358 0.657\n## 2 1             187 0.343\ntest_data %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))## # A tibble: 2 × 3\n##   test_result     n  prop\n##   <fct>       <int> <dbl>\n## 1 0             120 0.656\n## 2 1              63 0.344\n# Create a recipe\ndiabetes_rec <- \n  recipe(test_result ~ ., data = train_data)\n\n# Look at the recipe summary\nsummary(diabetes_rec)## # A tibble: 3 × 4\n##   variable    type    role      source  \n##   <chr>       <chr>   <chr>     <chr>   \n## 1 glucose     numeric predictor original\n## 2 diastolic   numeric predictor original\n## 3 test_result nominal outcome   original\ndiabetes_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\")\ndiabetes_wflow <- \n  workflow() %>% \n  add_model(diabetes_mod) %>% \n  add_recipe(diabetes_rec)\n\ndiabetes_wflow## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 0 Recipe Steps\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Logistic Regression Model Specification (classification)\n## \n## Computational engine: glm\ndiabetes_fit <- \n  diabetes_wflow %>% \n  fit(data = train_data)\ndiabetes_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()## # A tibble: 3 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)  -6.48     0.758       -8.55 1.21e-17\n## 2 glucose       0.0395   0.00407      9.71 2.64e-22\n## 3 diastolic     0.0127   0.00868      1.46 1.44e- 1\ndiabetes_aug <- \naugment(diabetes_fit, test_data)\n\ndiabetes_aug## # A tibble: 183 × 6\n##    glucose diastolic test_result .pred_class .pred_0 .pred_1\n##      <dbl>     <dbl> <fct>       <fct>         <dbl>   <dbl>\n##  1     116        74 0           0             0.724   0.276\n##  2     197        70 1           1             0.101   0.899\n##  3     168        74 1           1             0.251   0.749\n##  4     139        80 0           1             0.494   0.506\n##  5     189        60 1           1             0.149   0.851\n##  6     166        72 1           1             0.271   0.729\n##  7      99        84 0           0             0.819   0.181\n##  8     125        70 1           0             0.659   0.341\n##  9      97        66 0           0             0.860   0.140\n## 10     145        82 0           1             0.429   0.571\n## # … with 173 more rows\ndiabetes_aug %>% \n  roc_curve(truth = test_result, .pred_0) %>% \n  autoplot()\ndiabetes_aug %>% \n  filter(test_result == .pred_class) %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))## # A tibble: 2 × 3\n##   test_result     n  prop\n##   <fct>       <int> <dbl>\n## 1 0              97 0.724\n## 2 1              37 0.276\ndiabetes_aug %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))## # A tibble: 2 × 3\n##   test_result     n  prop\n##   <fct>       <int> <dbl>\n## 1 0             120 0.656\n## 2 1              63 0.344\ndiabetes_aug %>% \n  roc_auc(truth = test_result, .pred_0)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 roc_auc binary         0.777"},{"path":"logistic-models-binary-response.html","id":"split-the-data","chapter":"2 Logistic Models – Binary Response","heading":"2.7.1 Split the data","text":"created model, used data. However, good way assessing model fit actually split data two:training data set use fit modela test data set validate model measure model performanceBefore split data, let’s closer look data set. count many diabetes test results negative (0) positive (1), see counts evenly split.can consequences start splitting data training test set. splitting data two parts - data goes training set - data left afterwards can use test good predictions model . However, need make sure proportion negative positive diabetes test outcomes remains roughly .rsample package couple useful functions allow us just can use strata argument keep proportions less .can check initial_split() function done:","code":"\ndiabetes %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))## # A tibble: 2 × 3\n##   test_result     n  prop\n##   <fct>       <int> <dbl>\n## 1 0             478 0.657\n## 2 1             250 0.343\n# Use 75% of the data to create the training data set\ndata_split <- initial_split(diabetes, strata = test_result)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n# proportion of data allocated to the training set\nnrow(train_data) / nrow(diabetes)## [1] 0.7486264\n# proportion of diabetes test results for the training and test data sets\ntrain_data %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))## # A tibble: 2 × 3\n##   test_result     n  prop\n##   <fct>       <int> <dbl>\n## 1 0             358 0.657\n## 2 1             187 0.343\ntest_data %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))## # A tibble: 2 × 3\n##   test_result     n  prop\n##   <fct>       <int> <dbl>\n## 1 0             120 0.656\n## 2 1              63 0.344"},{"path":"logistic-models-binary-response.html","id":"create-a-recipe","chapter":"2 Logistic Models – Binary Response","heading":"2.7.2 Create a recipe","text":"","code":"\n# Create a recipe\ndiabetes_rec <- \n  recipe(test_result ~ ., data = train_data)\n\n# Look at the recipe summary\nsummary(diabetes_rec)## # A tibble: 3 × 4\n##   variable    type    role      source  \n##   <chr>       <chr>   <chr>     <chr>   \n## 1 glucose     numeric predictor original\n## 2 diastolic   numeric predictor original\n## 3 test_result nominal outcome   original"},{"path":"logistic-models-binary-response.html","id":"build-a-model-specification","chapter":"2 Logistic Models – Binary Response","heading":"2.7.3 Build a model specification","text":"","code":"\ndiabetes_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\")"},{"path":"logistic-models-binary-response.html","id":"use-recipe-as-we-train-and-test-our-model","chapter":"2 Logistic Models – Binary Response","heading":"2.7.4 Use recipe as we train and test our model","text":"Although seems bit overkill, now single function can can use prepare recipe train model resulting predictors:creates object called diabetes_fit, contains final recipe fitted model objects. can extract model recipe objects several helper functions:","code":"\ndiabetes_wflow <- \n  workflow() %>% \n  add_model(diabetes_mod) %>% \n  add_recipe(diabetes_rec)\n\ndiabetes_wflow## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 0 Recipe Steps\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Logistic Regression Model Specification (classification)\n## \n## Computational engine: glm\ndiabetes_fit <- \n  diabetes_wflow %>% \n  fit(data = train_data)\ndiabetes_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()## # A tibble: 3 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)  -6.48     0.758       -8.55 1.21e-17\n## 2 glucose       0.0395   0.00407      9.71 2.64e-22\n## 3 diastolic     0.0127   0.00868      1.46 1.44e- 1"},{"path":"logistic-models-binary-response.html","id":"use-trained-workflow-for-predictions","chapter":"2 Logistic Models – Binary Response","heading":"2.7.5 Use trained workflow for predictions","text":"far, done following:Built model (diabetes_mod),Created pre-processing recipe (diabetes_rec),Combined model recipe workflow (diabetes_wflow)Trained workflow using fit() function (diabetes_fit)results generated differ much values obtained entire data set. However, based 3/4 data (training data set). , still test data set available apply workflow data model yet seen.","code":"\ndiabetes_aug <- \naugment(diabetes_fit, test_data)\n\ndiabetes_aug## # A tibble: 183 × 6\n##    glucose diastolic test_result .pred_class .pred_0 .pred_1\n##      <dbl>     <dbl> <fct>       <fct>         <dbl>   <dbl>\n##  1     116        74 0           0             0.724   0.276\n##  2     197        70 1           1             0.101   0.899\n##  3     168        74 1           1             0.251   0.749\n##  4     139        80 0           1             0.494   0.506\n##  5     189        60 1           1             0.149   0.851\n##  6     166        72 1           1             0.271   0.729\n##  7      99        84 0           0             0.819   0.181\n##  8     125        70 1           0             0.659   0.341\n##  9      97        66 0           0             0.860   0.140\n## 10     145        82 0           1             0.429   0.571\n## # … with 173 more rows"},{"path":"logistic-models-binary-response.html","id":"evaluate-the-model","chapter":"2 Logistic Models – Binary Response","heading":"2.7.6 Evaluate the model","text":"can now evaluate model. One way using area ROC curve metric.","code":"\ndiabetes_aug %>% \n  roc_curve(truth = test_result, .pred_0) %>% \n  autoplot()\ndiabetes_aug %>% \n  filter(test_result == .pred_class) %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))## # A tibble: 2 × 3\n##   test_result     n  prop\n##   <fct>       <int> <dbl>\n## 1 0              97 0.724\n## 2 1              37 0.276\ndiabetes_aug %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))## # A tibble: 2 × 3\n##   test_result     n  prop\n##   <fct>       <int> <dbl>\n## 1 0             120 0.656\n## 2 1              63 0.344\ndiabetes_aug %>% \n  roc_auc(truth = test_result, .pred_0)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 roc_auc binary         0.777"},{"path":"logistic-models-binary-response.html","id":"exercise","chapter":"2 Logistic Models – Binary Response","heading":"2.8 Exercise","text":"Exercise 2.1  can add exercises","code":""},{"path":"logistic-models-binary-response.html","id":"key-points","chapter":"2 Logistic Models – Binary Response","heading":"2.9 Key points","text":"Adding key points","code":""}]
