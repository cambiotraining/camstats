[{},{"path":"index.html","id":"overview","chapter":"1 Overview","heading":"1 Overview","text":"sessions intended enable perform additional data analysis techniques appropriately confidently using R Python.Ongoing formative assessment exercisesOngoing formative assessment exercisesNo formal assessmentNo formal assessmentNo mathematical derivationsNo mathematical derivationsNo pen paper calculationsNo pen paper calculationsThey “mindlessly use stats program” course!","code":""},{"path":"index.html","id":"core-aims","chapter":"1 Overview","heading":"1.1 Core aims","text":"know presented non-standard dataset e.g.Know deal non-normal dataKnow analyse count dataBe able deal random effects","code":""},{"path":"index.html","id":"core-topics","chapter":"1 Overview","heading":"1.2 Core topics","text":"Generalised linear models","code":""},{"path":"index.html","id":"index-datasets","chapter":"1 Overview","heading":"1.3 Datasets","text":"course uses various data sets. easiest way accessing creating R-project RStudio. download data folder right-clicking link Save …. Next unzip file copy working directory. data accessible via <working-directory-name>/data.","code":""},{},{"path":"glm-intro.html","id":"glm-intro","chapter":"2 Introduction","heading":"2 Introduction","text":"","code":""},{"path":"glm-intro.html","id":"objectives","chapter":"2 Introduction","heading":"2.1 Objectives","text":"Aim: introduce R commands analysing data non-continuous response variables.end practical participants able achieve following:Construct\nlogistic model binary response variables\nlogistic model proportion response variables\nPoisson model count response variables\nNegative Binomial model count response variables\nlogistic model binary response variablesa logistic model proportion response variablesa Poisson model count response variablesa Negative Binomial model count response variablesPlot data fitted curve case continuous categorical predictorsAssess significance fitAssess assumption model","code":""},{"path":"glm-intro.html","id":"background","chapter":"2 Introduction","heading":"2.2 Background","text":"practical divided sections considers sort response variable generalised linear model turn. Within section least one example modelling process followed example..panelset{--panel-tab-font-family: inherit;}","code":""},{},{"path":"logistic-models-binary-response.html","id":"logistic-models-binary-response","chapter":"3 Logistic Models – Binary Response","heading":"3 Logistic Models – Binary Response","text":"","code":""},{"path":"logistic-models-binary-response.html","id":"objectives-1","chapter":"3 Logistic Models – Binary Response","heading":"3.1 Objectives","text":"QuestionsHow analyse data binary outcome?Can test model good?ObjectivesBe able perform logistic regression binary outcomePredict outcomes new data, based defined modelEvaluate model reliability using training test data sets","code":""},{"path":"logistic-models-binary-response.html","id":"libraries-and-functions","chapter":"3 Logistic Models – Binary Response","heading":"3.2 Libraries and functions","text":"tidyverse","code":""},{"path":"logistic-models-binary-response.html","id":"datasets","chapter":"3 Logistic Models – Binary Response","heading":"3.3 Datasets","text":"DiabetesThe example section uses following data set:data/diabetes.csvThis data set comprising 768 observations three variables (one dependent two predictor variables). records results diabetes test result binary variable (1 positive result, 0 negative result), along result glucose test diastolic blood pressure 767 women. variables called test_result, glucose diastolic.","code":""},{"path":"logistic-models-binary-response.html","id":"visualise-the-data","chapter":"3 Logistic Models – Binary Response","heading":"3.4 Visualise the data","text":"First load data, visualise . needed, load tidyverse package using:tidyverse\nFirst, load inspect data:Looking data, can see test_result column contains zeros ones. test result outcomes actually numeric representations.cause problems later, need tell R see values factors. good measure ’ll also improve information test_result classifying ‘negative’ (0) ‘positive’ (1).can plot data:looks though patients positive diabetes test slightly higher glucose levels negative diabetes test.can visualise differently plotting data points classic binary response plot:","code":"\ndiabetes <- read_csv(\"data/diabetes.csv\")\ndiabetes <- \ndiabetes %>% \n  # replace 0 with 'negative' and 1 with 'positive'\n  mutate(test_result = case_when(test_result == 0 ~ \"negative\",\n                                 TRUE ~ \"positive\")) %>% \n  # convert character columns to factor\n  mutate_if(is.character, factor)\ndiabetes %>% \n  ggplot(aes(x = test_result, y = glucose)) +\n  geom_boxplot()\ndiabetes %>% \n  ggplot(aes(x = glucose, y = test_result)) +\n  geom_point()"},{"path":"logistic-models-binary-response.html","id":"model-building","chapter":"3 Logistic Models – Binary Response","heading":"3.5 Model building","text":"different ways construct logistic model.tidyverseIn tidymodels access useful package: parsnip, provides common syntax whole range modelling libraries. means syntax stay different kind model comparisons. , learning curve might bit steeper start , pay dividend long-term (just like started using R!).First, need load tidymodels (install first, needed):workflow parsnip bit different ’re used far. now, ’ve directly used relevant model functions analyse data, example using lm() function create linear models.Using parsnip approach things systematic manner. first might seem unnecessarily verbose, clear advantages approaching analysis systematic way. example, straightforward implement types models using workflow, ’ll definitely find useful moving difficult modelling tasks.Using tidymodels specify model three steps:Specify type model based mathematical structure (e.g., linear regression, random forest, K-nearest neighbors, etc).required, declare mode model. mode reflects type prediction outcome. numeric outcomes, mode regression; qualitative outcomes, classification. model can create one type model, logistic regression, mode already set.Specify engine fitting model. usually software package library used., can create model follows:Note actually specifying variables just yet! ’ve done tell R kind model ’re planning use. want see parsnip converts code package syntax, can check translate():shows logistic regression model, outcome going classification (case, ’s positive negative test result). model fit template tells us ’ll using glm() function stats package, can take formula, data, weights family argument. family argument already set binomial.Now ’ve specified kind model ’re planning use, can fit data , using fit() function:can look output directly, prefer tidy data using tidy() function broom package:estimate column gives coefficients logistic model equation. use calculate probability positive diabetes test, given glucose level, using following equation:\\[\\begin{equation}\nP(positive \\ test \\ result) = \\frac{1}{1 + {e}^{-(-5.61 +  0.040 \\cdot glucose)}}\n\\end{equation}\\]course ’re going way. ’ll let R deal next section.std.error column gives error associated coefficients statistic column tells statistic value.values p.value merely show whether particular coefficient significantly different zero. similar p-values obtained summary output linear model, , continuous predictors p-values can used rough guide whether predictor important (case glucose appears significant). However, p-values aren’t great multiple predictor variables, categorical predictors multiple levels (since output give us p-value level rather predictor whole).","code":"\n# install.packages(\"tidymodels\")\nlibrary(tidymodels)\ndia_mod <- logistic_reg() %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"glm\")\ndia_mod %>% translate()## Logistic Regression Model Specification (classification)\n## \n## Computational engine: glm \n## \n## Model fit template:\n## stats::glm(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), \n##     family = stats::binomial)\ndia_fit <- dia_mod %>% \n  fit(test_result ~ glucose,\n      data = diabetes)\ndia_fit %>% tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)  -5.61     0.442       -12.7 6.90e-37\n## 2 glucose       0.0395   0.00340      11.6 2.96e-31"},{"path":"logistic-models-binary-response.html","id":"model-predictions","chapter":"3 Logistic Models – Binary Response","heading":"3.6 Model predictions","text":"got new glucose level data wanted predict people might diabetes ?use existing model feed data:tidyverseAlthough able get predicted outcomes (.pred_class), like stress point running model. important realise model (statistical models) creates predicted outcome based certain probabilities. therefore much informative look probable predicted outcomes . encoded .pred_negative .pred_positive.first value means 14% chance diabetes test return negative result around 86% chance return positive result.","code":"\n# create a dummy data set using some hypothetical glucose measurements\ndiabetes_newdata <- tibble(glucose = c(188, 122, 83, 76, 144))\n\n# predict if the patients have diabetes or not\naugment(dia_fit,\n        new_data = diabetes_newdata)## # A tibble: 5 × 4\n##   glucose .pred_class .pred_negative .pred_positive\n##     <dbl> <fct>                <dbl>          <dbl>\n## 1     188 positive             0.140         0.860 \n## 2     122 negative             0.688         0.312 \n## 3      83 negative             0.912         0.0885\n## 4      76 negative             0.931         0.0686\n## 5     144 positive             0.481         0.519"},{"path":"logistic-models-binary-response.html","id":"exercise-penguins","chapter":"3 Logistic Models – Binary Response","heading":"3.7 Exercise: Penguins","text":"Exercise 3.1  practice bit , ’ll using data set penguins. data palmerpenguins package, included tidymodels. data set contains information penguins Palmer Station Antarctica. Chilly.look plot , comparing bill length (bill_length_mm) three species penguins (species) flipper length (flipper_length_mm).also colouring data based sex (sex) good measure ’re also including information body size (body_mass_g).looks like female penguins smaller different sized bills interesting (yes, !) investigate .like following:load data object called penguins using data(\"penguins\")create logistic model fit data , using sex classifieris bill length important indicator sex?First, load data:already reasonably good idea ’re looking , can never hurt understand data better, :shows columns data set, namely island, indicating island penguins residing bill_depth_mm records bill depth.also notice missing values. good get rid , least rows sex isn’t scored:Next, specify type model. Notice can useful use prefix naming objects indicate data set model belongs . ’re using pgn denote penguins.Remember, setting model specification yet define model . follows:’ve fitted data model, can look model parameters:model parameters tell us intercept coefficient bill_length_mm significantly different zero. seems bill length important predictor sex penguins. knew?!","code":"\ndata(\"penguins\")\nhead(penguins)## # A tibble: 6 × 7\n##   species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  \n##   <fct>   <fct>           <dbl>         <dbl>            <int>       <int> <fct>\n## 1 Adelie  Torge…           39.1          18.7              181        3750 male \n## 2 Adelie  Torge…           39.5          17.4              186        3800 fema…\n## 3 Adelie  Torge…           40.3          18                195        3250 fema…\n## 4 Adelie  Torge…           NA            NA                 NA          NA <NA> \n## 5 Adelie  Torge…           36.7          19.3              193        3450 fema…\n## 6 Adelie  Torge…           39.3          20.6              190        3650 male\npenguins <- penguins %>% \n  filter(!is.na(sex))\npgn_mod <- logistic_reg() %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"glm\")\npgn_fit <- pgn_mod %>% \n  fit(sex ~ bill_length_mm,\n      data = penguins)\npgn_fit %>% tidy()## # A tibble: 2 × 5\n##   term           estimate std.error statistic       p.value\n##   <chr>             <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)      -6.04     1.01       -5.96 0.00000000247\n## 2 bill_length_mm    0.138    0.0229      6.02 0.00000000176"},{"path":"logistic-models-binary-response.html","id":"model-evaluation","chapter":"3 Logistic Models – Binary Response","heading":"3.8 Model evaluation","text":"far ’ve constructed logistic model fed new data make predictions possible outcome diabetes test, depending glucose level given patient. gave us diabetes test predictions , importantly, probabilities whether test come back negative positive.question ’d like ask point: reliable model?explore , need take step back.","code":""},{"path":"logistic-models-binary-response.html","id":"split-the-data","chapter":"3 Logistic Models – Binary Response","heading":"3.8.1 Split the data","text":"created model, used data. However, good way assessing model fit actually split data two:training data set use fit modela test data set validate model measure model performanceBefore split data, let’s closer look data set. count many diabetes test results negative positive, see counts evenly split.tidyverseThis can consequences start splitting data training test set. splitting data two parts - data goes training set - data left afterwards can use test good predictions model . However, need make sure proportion negative positive diabetes test outcomes remains roughly .rsample package couple useful functions allow us just can use strata argument keep proportions less constant.can check initial_split() function done:output can see around 75% data set used create training data set, remaining 25% kept test set.Furthermore, proportions negative:positive kept less constant.Although seems bit overkill, now single function can can use prepare recipe train model resulting predictors:creates object called dia_fit, contains final recipe fitted model objects. can extract model recipe objects several helper functions:far, done following:Built model (dia_mod),Created pre-processing recipe (dia_rec),Combined model recipe workflow (dia_wflow)Trained workflow using fit() function (dia_fit)results generated differ much values obtained entire data set. However, based 3/4 data (training data set). , still test data set available apply workflow data model yet seen.can now evaluate model. One way using area ROC curve metric.ROC curve (receiver operating characteristic curve - name strange relic WWII developed operators military radar receivers) plots true-positive rate (TPR) false-positive rate (FPR) varying thresholds.true-positive rate also known sensitivity, whereas false-positive rate 1 - sensitivity (, recall session Power Analysis also known power.)area ROC curve, known AUC provides aggregate measure performance across possible classification thresholds.ranges value 0 1. model whose predictions 100% wrong AUC 0. model whose predictions 100% correct AUC 1.0.addition ROC curve AUC also whole range model parameters associated fitted model. ’re going point, one particular familiar.extract parameters follows:see Akaike Information Criterion (AIC) output. Remember, value AIC meaningless, ’s useful compare relative AICs models. covered Power analysis session Core statistics course.see AIC model uses glucose level single predictor diabetes test result 558.","code":"\ndiabetes %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))## # A tibble: 2 × 3\n##   test_result     n  prop\n##   <fct>       <int> <dbl>\n## 1 negative      478 0.657\n## 2 positive      250 0.343\n# ensures random data split is reproducible\nset.seed(123)\n\n# split the data, basing the proportions on the diabetes test results\ndata_split <- initial_split(diabetes, strata = test_result)\n\n# create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n# proportion of data allocated to the training set\nnrow(train_data) / nrow(diabetes)## [1] 0.7486264\n# proportion of diabetes test results for the training data set\ntrain_data %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))## # A tibble: 2 × 3\n##   test_result     n  prop\n##   <fct>       <int> <dbl>\n## 1 negative      358 0.657\n## 2 positive      187 0.343\n# proportion of diabetes test results for the test data set\ntest_data %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))## # A tibble: 2 × 3\n##   test_result     n  prop\n##   <fct>       <int> <dbl>\n## 1 negative      120 0.656\n## 2 positive       63 0.344\n# Create a recipe\ndia_rec <- \n  recipe(test_result ~ glucose, data = train_data)\n\n# Look at the recipe summary\nsummary(dia_rec)## # A tibble: 2 × 4\n##   variable    type    role      source  \n##   <chr>       <chr>   <chr>     <chr>   \n## 1 glucose     numeric predictor original\n## 2 test_result nominal outcome   original\ndia_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\")\ndia_wflow <- \n  workflow() %>% \n  add_model(dia_mod) %>% \n  add_recipe(dia_rec)\n\ndia_wflow## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 0 Recipe Steps\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Logistic Regression Model Specification (classification)\n## \n## Computational engine: glm\ndia_fit <- \n  dia_wflow %>% \n  fit(data = train_data)\ndia_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)  -5.72     0.513       -11.2 6.84e-29\n## 2 glucose       0.0406   0.00397      10.2 1.46e-24\ndia_aug <- \naugment(dia_fit, test_data)\n\ndia_aug## # A tibble: 183 × 6\n##    glucose diastolic test_result .pred_class .pred_negative .pred_positive\n##      <dbl>     <dbl> <fct>       <fct>                <dbl>          <dbl>\n##  1      85        66 negative    negative            0.906          0.0938\n##  2     183        64 positive    positive            0.152          0.848 \n##  3     168        74 positive    positive            0.249          0.751 \n##  4     166        72 positive    positive            0.264          0.736 \n##  5     115        70 positive    negative            0.740          0.260 \n##  6      99        84 negative    negative            0.845          0.155 \n##  7     196        90 positive    positive            0.0959         0.904 \n##  8     119        80 positive    negative            0.708          0.292 \n##  9     143        94 positive    positive            0.478          0.522 \n## 10      97        66 negative    negative            0.856          0.144 \n## # … with 173 more rows\ndia_aug %>% \n  roc_curve(truth = test_result, .pred_negative) %>% \n  autoplot()\ndia_aug %>% \n  roc_auc(truth = test_result, .pred_negative)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 roc_auc binary         0.766\ndia_fit %>% glance()## # A tibble: 1 × 8\n##   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n##           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n## 1          701.     544  -277.  558.  567.     554.         543   545"},{"path":"logistic-models-binary-response.html","id":"create-a-recipe","chapter":"3 Logistic Models – Binary Response","heading":"3.8.2 Create a recipe","text":"","code":"\n# Create a recipe\ndia_rec <- \n  recipe(test_result ~ glucose, data = train_data)\n\n# Look at the recipe summary\nsummary(dia_rec)## # A tibble: 2 × 4\n##   variable    type    role      source  \n##   <chr>       <chr>   <chr>     <chr>   \n## 1 glucose     numeric predictor original\n## 2 test_result nominal outcome   original"},{"path":"logistic-models-binary-response.html","id":"build-a-model-specification","chapter":"3 Logistic Models – Binary Response","heading":"3.8.3 Build a model specification","text":"","code":"\ndia_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\")"},{"path":"logistic-models-binary-response.html","id":"use-recipe-as-we-train-and-test-our-model","chapter":"3 Logistic Models – Binary Response","heading":"3.8.4 Use recipe as we train and test our model","text":"Although seems bit overkill, now single function can can use prepare recipe train model resulting predictors:creates object called dia_fit, contains final recipe fitted model objects. can extract model recipe objects several helper functions:","code":"\ndia_wflow <- \n  workflow() %>% \n  add_model(dia_mod) %>% \n  add_recipe(dia_rec)\n\ndia_wflow## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 0 Recipe Steps\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Logistic Regression Model Specification (classification)\n## \n## Computational engine: glm\ndia_fit <- \n  dia_wflow %>% \n  fit(data = train_data)\ndia_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)  -5.72     0.513       -11.2 6.84e-29\n## 2 glucose       0.0406   0.00397      10.2 1.46e-24"},{"path":"logistic-models-binary-response.html","id":"use-trained-workflow-for-predictions","chapter":"3 Logistic Models – Binary Response","heading":"3.8.5 Use trained workflow for predictions","text":"far, done following:Built model (dia_mod),Created pre-processing recipe (dia_rec),Combined model recipe workflow (dia_wflow)Trained workflow using fit() function (dia_fit)results generated differ much values obtained entire data set. However, based 3/4 data (training data set). , still test data set available apply workflow data model yet seen.","code":"\ndia_aug <- \naugment(dia_fit, test_data)\n\ndia_aug## # A tibble: 183 × 6\n##    glucose diastolic test_result .pred_class .pred_negative .pred_positive\n##      <dbl>     <dbl> <fct>       <fct>                <dbl>          <dbl>\n##  1      85        66 negative    negative            0.906          0.0938\n##  2     183        64 positive    positive            0.152          0.848 \n##  3     168        74 positive    positive            0.249          0.751 \n##  4     166        72 positive    positive            0.264          0.736 \n##  5     115        70 positive    negative            0.740          0.260 \n##  6      99        84 negative    negative            0.845          0.155 \n##  7     196        90 positive    positive            0.0959         0.904 \n##  8     119        80 positive    negative            0.708          0.292 \n##  9     143        94 positive    positive            0.478          0.522 \n## 10      97        66 negative    negative            0.856          0.144 \n## # … with 173 more rows"},{"path":"logistic-models-binary-response.html","id":"evaluate-the-model","chapter":"3 Logistic Models – Binary Response","heading":"3.8.6 Evaluate the model","text":"can now evaluate model. One way using area ROC curve metric.ROC curve (receiver operating characteristic curve - name strange relic WWII developed operators military radar receivers) plots true-positive rate (TPR) false-positive rate (FPR) varying thresholds.true-positive rate also known sensitivity, whereas false-positive rate 1 - sensitivity (, recall session Power Analysis also known power.)area ROC curve, known AUC provides aggregate measure performance across possible classification thresholds.ranges value 0 1. model whose predictions 100% wrong AUC 0. model whose predictions 100% correct AUC 1.0.addition ROC curve AUC also whole range model parameters associated fitted model. ’re going point, one particular familiar.extract parameters follows:see Akaike Information Criterion (AIC) output. Remember, value AIC meaningless, ’s useful compare relative AICs models. covered Power analysis session Core statistics course.see AIC model uses glucose level single predictor diabetes test result 558.","code":"\ndia_aug %>% \n  roc_curve(truth = test_result, .pred_negative) %>% \n  autoplot()\ndia_aug %>% \n  roc_auc(truth = test_result, .pred_negative)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 roc_auc binary         0.766\ndia_fit %>% glance()## # A tibble: 1 × 8\n##   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n##           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n## 1          701.     544  -277.  558.  567.     554.         543   545"},{"path":"logistic-models-binary-response.html","id":"exercise-diabetes-predictors","chapter":"3 Logistic Models – Binary Response","heading":"3.9 Exercise: Diabetes predictors","text":"Exercise 3.2  Using training test diabetes data sets, investigate relationship test_result glucose diastolic. Try answer following:adding diastolic model markedly improve reliability predictions?AICs two models tell ?Build model, needed (done already stays ):Create workflow…… fit data:Extract model parameters look:Apply fitted model test data set:Plot ROC curve:get area ROC curve:Another way assess model fit look Akaike Information Criterion (AIC).get AIC 555, lower AIC 558 got just glucose predictor variable.Adding diastolic variable predictor model seem much effect model reliability, since AUC 0.761 extra parameter, versus 0.766 without.AIC hand suggests additive model ’ve analysed better fit original model (AIC 555 vs 558).Perhaps interaction glucose diastolic, interesting investigate.","code":"\n# Update the recipe\ndia_rec <- \n  recipe(test_result ~ glucose + diastolic,\n         data = train_data)\n\n# Look at the recipe summary\nsummary(dia_rec)## # A tibble: 3 × 4\n##   variable    type    role      source  \n##   <chr>       <chr>   <chr>     <chr>   \n## 1 glucose     numeric predictor original\n## 2 diastolic   numeric predictor original\n## 3 test_result nominal outcome   original\ndia_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\")\ndia_wflow <- \n  workflow() %>% \n  add_model(dia_mod) %>% \n  add_recipe(dia_rec)\ndia_fit <- \n  dia_wflow %>% \n  fit(data = train_data)\ndia_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()## # A tibble: 3 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)  -6.99     0.790       -8.85 8.60e-19\n## 2 glucose       0.0394   0.00398      9.88 5.19e-23\n## 3 diastolic     0.0195   0.00877      2.22 2.61e- 2\ndia_aug <- \naugment(dia_fit, test_data)\n\ndia_aug## # A tibble: 183 × 6\n##    glucose diastolic test_result .pred_class .pred_negative .pred_positive\n##      <dbl>     <dbl> <fct>       <fct>                <dbl>          <dbl>\n##  1      85        66 negative    negative            0.914          0.0862\n##  2     183        64 positive    positive            0.189          0.811 \n##  3     168        74 positive    positive            0.257          0.743 \n##  4     166        72 positive    positive            0.280          0.720 \n##  5     115        70 positive    negative            0.751          0.249 \n##  6      99        84 negative    negative            0.811          0.189 \n##  7     196        90 positive    positive            0.0776         0.922 \n##  8     119        80 positive    negative            0.679          0.321 \n##  9     143        94 positive    positive            0.385          0.615 \n## 10      97        66 negative    negative            0.869          0.131 \n## # … with 173 more rows\ndia_aug %>% \n  roc_curve(truth = test_result, .pred_negative) %>% \n  autoplot()\ndia_aug %>% \n  roc_auc(truth = test_result, .pred_negative)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 roc_auc binary         0.761\ndia_fit %>% glance()## # A tibble: 1 × 8\n##   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n##           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n## 1          701.     544  -275.  555.  568.     549.         542   545"},{"path":"logistic-models-binary-response.html","id":"conclusions","chapter":"3 Logistic Models – Binary Response","heading":"3.9.0.1 Conclusions","text":"Adding diastolic variable predictor model seem much effect model reliability, since AUC 0.761 extra parameter, versus 0.766 without.AIC hand suggests additive model ’ve analysed better fit original model (AIC 555 vs 558).","code":""},{"path":"logistic-models-binary-response.html","id":"food-for-thought","chapter":"3 Logistic Models – Binary Response","heading":"3.9.0.2 Food for thought","text":"Perhaps interaction glucose diastolic, interesting investigate.","code":""},{"path":"logistic-models-binary-response.html","id":"key-points","chapter":"3 Logistic Models – Binary Response","heading":"3.10 Key points","text":"use logistic regression model binary responseModel suitability can checked splitting data training test data set. logistic model created based training data, reliability can checked (known) values test data setThe ROC curve shows performance classification model thresholds, whereas area ROC curve provides aggregate measure performance possible classifications thresholds.panelset{--panel-tab-font-family: inherit;}","code":""},{},{"path":"logistic-regression---proportion-response.html","id":"logistic-regression---proportion-response","chapter":"4 Logistic regression - proportion response","heading":"4 Logistic regression - proportion response","text":"","code":""},{"path":"logistic-regression---proportion-response.html","id":"objectives-2","chapter":"4 Logistic regression - proportion response","heading":"4.1 Objectives","text":"QuestionsHow analyse proportion responses?ObjectivesBe able create logistic model test proportion response variablesBe able plot data fitted curveAssess significance fit","code":""},{"path":"logistic-regression---proportion-response.html","id":"libraries-and-functions-1","chapter":"4 Logistic regression - proportion response","heading":"4.2 Libraries and functions","text":"tidyverse","code":""},{"path":"logistic-regression---proportion-response.html","id":"datasets-1","chapter":"4 Logistic regression - proportion response","heading":"4.3 Datasets","text":"DiabetesThe example section uses following data set:data/challenger.csvThese data, obtained faraway package, contain information related explosion USA Space Shuttle Challenger 28 January, 1986. investigation disaster traced back certain joints one two solid booster rockets, containing two O-rings (primary secondary) ensured exhaust gases escape booster.night launch unusually cold, temperatures freezing. final report suggested cold snap night made o-rings stiff, unable adjust changes pressure. result, exhaust gases leaked away solid booster rockets, causing one break loose rupture main fuel tank, leading final explosion.question ’re trying answer session : based data previous flights, possible predict failure o-rings Challenger flight?","code":""},{"path":"logistic-regression---proportion-response.html","id":"visualise-the-data-1","chapter":"4 Logistic regression - proportion response","heading":"4.4 Visualise the data","text":"First, read data:tidyverseThe data set contains several columns:temp, launch temperature degrees Fahrenheitdamage, number o-rings showed erosionBefore look data, let’s calculate proportion damaged o-rings (prop_damaged) total number o-rings (total) update data set.tidyversePlotting proportion damaged o-rings launch temperature shows following picture:tidyverseThe point left data point corresponding coldest flight experienced disaster, five damaged o-rings found. Fortunately, result disaster.’ll explore predicted failure o-rings Challenger flight, launch temperature 31 degrees Fahrenheit.","code":"\nchallenger <- read_csv(\"data/challenger.csv\")## Rows: 23 Columns: 2\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl (2): temp, damage\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nchallenger## # A tibble: 23 × 2\n##     temp damage\n##    <dbl>  <dbl>\n##  1    53      5\n##  2    57      1\n##  3    58      1\n##  4    63      1\n##  5    66      0\n##  6    67      0\n##  7    67      0\n##  8    67      0\n##  9    68      0\n## 10    69      0\n## # … with 13 more rows\nchallenger <-\nchallenger %>%\n  mutate(total = 6,                     # total number of o-rings\n         intact = 6 - damage,           # number of undamaged o-rings\n         prop_damaged = damage / total) # proportion damaged o-rings\n\nchallenger## # A tibble: 23 × 5\n##     temp damage total intact prop_damaged\n##    <dbl>  <dbl> <dbl>  <dbl>        <dbl>\n##  1    53      5     6      1        0.833\n##  2    57      1     6      5        0.167\n##  3    58      1     6      5        0.167\n##  4    63      1     6      5        0.167\n##  5    66      0     6      6        0    \n##  6    67      0     6      6        0    \n##  7    67      0     6      6        0    \n##  8    67      0     6      6        0    \n##  9    68      0     6      6        0    \n## 10    69      0     6      6        0    \n## # … with 13 more rows\nggplot(challenger, aes(x = temp, y = prop_damaged)) +\n  geom_point()"},{"path":"logistic-regression---proportion-response.html","id":"model-building-1","chapter":"4 Logistic regression - proportion response","heading":"4.5 Model building","text":"little point evaluating model using training/test data set, since 23 data points total. ’re building model testing available data.tidyverse\nusing logistic regression proportion response case, since ’re interested proportion o-rings damaged.logistic_reg() function used binary response section work , expects binary (yes/; positive/negative; 0/1 etc) response.deal , using standard linear_reg() function, still using glm generalised linear model engine, family error distribution set binomial ().First set model specification:fit data. Fitting data proportion responses bit annoying, give glm model two-column matrix specify response variable., first column corresponds number damaged o-rings, whereas second column refers number intact o-rings. use cbind() function bind two together matrix.Next, can closer look results:can see p-values intercept temp significant. can also use intercept temp coefficients construct logistic equation, can use sketch logistic curve.\\[\\begin{equation}\nP(o-ring \\ failure) = \\frac{1}{1 + {e}^{-(11.66 -  0.22 \\cdot temp)}}\n\\end{equation}\\]Let’s see well model performed fed data ill-fated Challenger launch.First generate table data range temperatures, 25 85 degrees Fahrenheit, steps 1. can use data generate logistic curve, based fitted model.seems high probability o-rings failing launch temperature. One thing graph shows lot uncertainty involved model.","code":"\nchl_mod <- linear_reg(mode = \"regression\") %>%\n  set_engine(\"glm\", family = \"binomial\")\nchl_fit <- chl_mod %>% \n  fit(cbind(damage, intact) ~ temp,\n      data = challenger)\nchl_fit %>% tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)   11.7      3.30        3.54 0.000403 \n## 2 temp          -0.216    0.0532     -4.07 0.0000478\nmodel <- tibble(temp = seq(25, 85, 1))\n# get the predicted proportions for the curve\ncurve <- chl_fit %>% augment(new_data = model)\n\n# plot the curve and the original data\nggplot(curve, aes(temp, .pred)) +\n  geom_line(colour = \"red\") +\n  geom_point(data = challenger, aes(temp, prop_damaged)) +\n  # add a vertical line at the disaster launch temperature\n  geom_vline(xintercept = 31, linetype = \"dashed\")"},{"path":"logistic-regression---proportion-response.html","id":"exercise","chapter":"4 Logistic regression - proportion response","heading":"4.6 Exercise","text":"Exercise 4.1  data point 53 degrees Fahrenheit quite influential analysis. Remove data point repeat analysis. still predicted link launch temperature o-ring failure?tidyverseFirst, need remove influential data point:can reuse model specification, update fit:prediction proportion damaged o-rings markedly less scenario, failure rate around 80%. original fitted curve already quite uncertainty associated , uncertainty model much greater.","code":"\nchallenger_new <- challenger %>% filter(temp != 53)\nchl_new_fit <- chl_mod %>% \n  fit(cbind(damage, intact) ~ temp,\n      data = challenger_new)\n# get the predicted proportions for the curve\ncurve_new <- chl_new_fit %>% augment(new_data = model)\n\n# plot the curve and the original data\nggplot(curve_new, aes(temp, .pred)) +\n  geom_line(colour = \"red\") +\n  geom_point(data = challenger_new, aes(temp, prop_damaged)) +\n  # add a vertical line at the disaster launch temperature\n  geom_vline(xintercept = 31, linetype = \"dashed\")"},{"path":"logistic-regression---proportion-response.html","id":"key-points-1","chapter":"4 Logistic regression - proportion response","heading":"4.7 Key points","text":"can use logistic model proportion response variables","code":""},{},{"path":"resampling-intro.html","id":"resampling-intro","chapter":"5 Introduction","heading":"5 Introduction","text":"","code":""},{"path":"resampling-intro.html","id":"objectives-3","chapter":"5 Introduction","heading":"5.1 Objectives","text":"Aim: introduce R/Python commands algorithms conducting simple permutation tests.end practical participants able performMonte Carlo permutation tests \nTwo-samples continuous data\nMultiple samples continuous data\nSimple Linear Regression\nTwo-way anova\nTwo-samples continuous dataMultiple samples continuous dataSimple Linear RegressionTwo-way anovaand understand apply techniques generally.","code":""},{"path":"resampling-intro.html","id":"background-1","chapter":"5 Introduction","heading":"5.2 Background","text":"traditional statistical test make use various named distributions (normally normal distribution, lol, parametric tests like t-test ANOVA) order work properly, require certain assumptions made parent distribution (shape distribution symmetric non-parametric tests like Wilcoxon). assumptions met traditional statistical tests fine, can can’t assume normality distribution data just weird?Resampling techniques tools work . can allow us test hypotheses data using data (without appeal assumptions shape form parent distribution). ways much simpler approach statistics, rely ability generate thousands tens thousands random numbers quickly, simply weren’t considered practical back day. Even now, aren’t widely used require user (, case ’d forgotten ’s going time day) click button stats package even know name test. techniques require mix statistical knowledge programming; combination skills isn’t common! three broad areas resampling methods (although quite closely related):Permutation MethodsBootstrappingCross-validationPermutation methods focus practical allow us carry hypothesis testing.Bootstrapping technique estimating confidence intervals parameter estimates. effectively treat dataset parent distribution, draw samples calculate statistic choice (mean usually) using sub-samples. repeat process many times, eventually able construct distribution sample statistic. can used give us confidence interval statistic.Cross-validation heart modern machine learning approaches existed long technique became sexy/fashionable. divide dataset two sets: training set use fit model testing set use evaluate model. allows model accuracy empirically measured. several variants technique (holdout, k-fold cross validation, leave-one--cross-validation (LOOCV), leave-p--cross-validation (LpOCV) etc.), essentially thing; main difference trade-amount time takes perform versus reliability method.won’t cover bootstrapping cross-validation practical feel free Google ..panelset{--panel-tab-font-family: inherit;}","code":"## Warning: 'xaringanExtra::style_panelset' is deprecated.\n## Use 'style_panelset_tabs' instead.\n## See help(\"Deprecated\")"},{},{"path":"single-predictor-permutation-tests.html","id":"single-predictor-permutation-tests","chapter":"6 Single predictor permutation tests","heading":"6 Single predictor permutation tests","text":"","code":""},{"path":"single-predictor-permutation-tests.html","id":"objectives-4","chapter":"6 Single predictor permutation tests","heading":"6.1 Objectives","text":"ObjectivesUnderstand resampling techniques work RBe able carry permutation techniques single predictorsBe able define statistic permuteUnderstand advantages limitations permutation techniques","code":""},{"path":"single-predictor-permutation-tests.html","id":"libraries-and-functions-2","chapter":"6 Single predictor permutation tests","heading":"6.2 Libraries and functions","text":"tidyverse","code":""},{"path":"single-predictor-permutation-tests.html","id":"purpose-and-aim","chapter":"6 Single predictor permutation tests","heading":"6.3 Purpose and aim","text":"wish test difference two groups case assumptions two-sample t-test just aren’t met, two-sample permutation test procedure appropriate. also appropriate even assumptions t-test met, case, easier just t-test.One additional benefits permutation test aren’t just restricted testing hypotheses means two groups. can test hypotheses absolutely anything want! , see ranges two groups differed significantly etc.","code":""},{"path":"single-predictor-permutation-tests.html","id":"data-and-hypotheses","chapter":"6 Single predictor permutation tests","heading":"6.4 Data and hypotheses","text":"Let’s consider experimental data set measured weights two groups 12 female mice (24 mice total). One group mice given perfectly normal diet (control) group mice given high fat diet several months. want test whether difference mean weight two groups. still need specify hypotheses:\\(H_0\\): difference means two groups\\(H_1\\): difference means two groups","code":""},{"path":"single-predictor-permutation-tests.html","id":"load-and-visualise-the-data","chapter":"6 Single predictor permutation tests","heading":"6.4.1 Load and visualise the data","text":"First load data, visualise .tidyverseIt looks mice fed high fat diet greater weight control (hardly surprising!). look bit closely calculate difference mean weight two groups:Let’s store value object called mice_diff.Right, difference two group means 3.02, hoorah! difference lot? unusual/big/statistically significant?Specifically, likely get difference big difference two groups? Let’s find !","code":"\n# load the data\nmice <- read_csv(\"data/mice.csv\")\n\n# view the data\nmice## # A tibble: 24 × 2\n##    diet    weight\n##    <chr>    <dbl>\n##  1 control   21.5\n##  2 control   28.1\n##  3 control   24.0\n##  4 control   23.4\n##  5 control   23.7\n##  6 control   19.8\n##  7 control   28.4\n##  8 control   21.0\n##  9 control   22.5\n## 10 control   20.1\n## # … with 14 more rows\nggplot(mice, aes(x = diet, y = weight)) +\n  geom_boxplot()\n# determine mean weight per group\nmice %>% \n  group_by(diet) %>%                         # split data by diet\n  summarise(mean_weight = mean(weight)) %>%  # calculate mean weight per group\n  ungroup() %>%                              # remove the grouping\n  pull(mean_weight) %>%                      # extract group values\n  diff()                                     # calculate the difference## [1] 3.020833"},{"path":"single-predictor-permutation-tests.html","id":"permutation-test-theory","chapter":"6 Single predictor permutation tests","heading":"6.5 Permutation Test Theory","text":"key idea behind permutation techniques null hypothesis true, difference two groups switch mice one group next wouldn’t change difference groups much. hand actually difference groups (one group much higher weights ), switch mice groups average two groups leading smaller difference group means., shuffle mice weights around lots lots times, calculating difference group means time. done shuffling hundreds thousands times, loads possible values difference two group means. stage can look actual difference (one calculated original data) see compares simulated differences.\ncan calculate many simulated differences bigger real difference proportion exactly p-value ’re looking !\nLet look carry practice.tidyversebase RtidyverseTo get better sense reliable p-value might , repeat whole process many times obtain resulting p-values.One way getting p-value single iteration follows:want repeat iterations many times can wrap whole workflow used obtain p-value within replicate() function tell many times want repeat . repeat whole process 100 times:might get warning reporting p-value zero. depends number reps chosen generate() function. ’s low , due simulation-based nature package observed statistic extreme test statistic generated form null hypothesis. happens, approximate p-value zero. Ending warning, true p-value zero impossible.","code":"\nset.seed(123)\n\nmice_resample <- mice %>% \n  specify(weight ~ diet) %>% \n  hypothesise(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(\"diff in means\", order = c(\"control\", \"highfat\"))\n\nmice_resample %>% \n  visualise() +\n  shade_p_value(obs_stat = mice_diff, direction = \"two-sided\")\nset.seed(123)\nreps<-1000\nsim_diff<-numeric(reps)\nfor(i in 1:reps){\n  new_dat<-mice\n  new_dat$diet<-sample(new_dat$diet)\n  new_means <- aggregate(weight ~ diet , new_dat , mean)$weight\n  new_diff <- diff(new_means)\n  \n  sim_diff[i]<-new_diff  \n}\nhist(sim_diff , breaks = 30 , col=\"red\")\nabline(v = mice_diff , col=\"black\" , lwd=2)\n# get a two-tailed p-value\np_value <- mice_resample %>%\n  get_p_value(obs_stat = mice_diff, direction = \"two-sided\")\n\np_value## # A tibble: 1 × 1\n##   p_value\n##     <dbl>\n## 1   0.074\n# remove the set.seed()\n# otherwise we get the same result 100 times\nset.seed(NULL)\n\nresample_replicates <- replicate(100, mice %>% \n  specify(weight ~ diet) %>% \n  hypothesise(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(\"diff in means\", order = c(\"control\", \"highfat\")) %>% \n  get_p_value(obs_stat = mice_diff, direction = \"two-sided\") %>% \n  pull()) %>% \n  as_tibble() %>% \n  mutate(n_rep = 1:n(),\n         p_value = value) %>% \n  select(-value)\n\nggplot(resample_replicates, aes(x = p_value)) +\n  geom_histogram(bins = 10)"},{"path":"single-predictor-permutation-tests.html","id":"exercise-rats-on-a-wheel","chapter":"6 Single predictor permutation tests","heading":"6.6 Exercise: Rats on a wheel","text":"Exercise 6.1  data set data/rats.csv contains information length time 24 rats able stay balanced rotating wheel. 12 rats assigned control group 12 given dose centrally acting muscle relaxant. animals placed rotating cylinder length time rat remained cylinder measured, maximum 300 seconds. data set contains two variables time group.\nWhilst explore differences means two groups, case alternative statistic presents . look data notice control group 12 rats manage stay roller maximum 300 seconds, whereas treated group 5 12 fall earlier.exercise, instead calculating mean length time group, calculate proportion rats make 300s group find difference. statistic.Use permutation test decide whether proportion rats survive group.tidyverse\nlook stat options calculate() functiontidyverseAs always, let’s first load visualise data:lot overlap values (many rats manage stay wheel entire 300s), need jitter data bit.’re interested proportion rats make full 300s. , let’s calculate :, means proportion rats make full-time follows:Now, question , difference proportion likely ? check , resample data see likely proportional difference observe .answer : likely. One thing keep mind ’ve resampled thousand times . ’s really fair, since thousand different options possible due low sample size. However, just means responses occur often. able calculate exactly, without using resampling, bit headache. Importantly, really use technique much samples, ’s good illustration can use technique analyse different statistics.put number , can get p-value like :base R","code":"\nrats <- read_csv(\"data/rats.csv\")\n\nrats## # A tibble: 24 × 2\n##     time group  \n##    <dbl> <chr>  \n##  1   300 control\n##  2   300 control\n##  3   300 control\n##  4   300 control\n##  5   300 control\n##  6   300 control\n##  7   300 control\n##  8   300 control\n##  9   300 control\n## 10   300 control\n## # … with 14 more rows\nrats %>% \n  ggplot(aes(x = group, y = time)) +\n  geom_jitter(width = 0.1)\nrats <- rats %>% \n  group_by(group) %>% \n  mutate(full_time = time == 300,\n         full_time = as.character(full_time))\nfull_time_control = 12/12\nfull_time_treatment = 7/12\n\nrats_diff <- full_time_control - full_time_treatment\nset.seed(123)\nrats_resample <- rats %>% \n  specify(full_time ~ group, success = \"TRUE\") %>% \n  hypothesise(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(\"diff in props\", order = c(\"control\", \"treatment\"))\n\nrats_resample %>% \n  visualise() +\n  shade_p_value(obs_stat = rats_diff, direction = \"two-sided\")\n# get a two-tailed p-value\np_value <- rats_resample %>%\n  get_p_value(obs_stat = rats_diff, direction = \"two-sided\")\n\np_value## # A tibble: 1 × 1\n##   p_value\n##     <dbl>\n## 1   0.038\nset.seed(123)\nrats_r <- read.csv(\"data/rats.csv\")\n\nunstRats<-unstack(rats_r)\n\npropControl <- length(\n  which(unstRats$control==300)) / length(unstRats$control)\n\npropTreatment <- length(\n  which(unstRats$treatment==300)) / length(unstRats$treatment)\n\nratDiff <- propControl - propTreatment\n\nnReps <- 1000\nsimRat<-numeric(nReps)\n\nfor(i in 1:nReps){\n  \n  newdat <- rats_r\n  newdat$group <- sample(newdat$group)\n  \n  newUnstRats <- unstack(newdat)\n  \n  newPropControl <- length(\n    which(newUnstRats$control==300))/length(newUnstRats$control)\n  \n  newPropTreatment <- length(\n    which(newUnstRats$treatment==300))/length(newUnstRats$treatment)\n  \n  newDiff <- newPropControl - newPropTreatment\n  \n  simRat[i] <- newDiff\n}\n\nhist(simRat, breaks = 30, col='red' , main=\"\" , xlab=\"Simulated Differences\")\nabline(v = ratDiff, col = \"black\", lwd = 2)\nabline(v = -ratDiff, col = \"black\", lwd = 2)"},{"path":"single-predictor-permutation-tests.html","id":"resampling-based-on-a-linear-regression","chapter":"6 Single predictor permutation tests","heading":"6.7 Resampling based on a linear regression","text":"far ’ve seen two examples can use permutation techniques look data: looking difference means (mice---diet example) comparing difference proportion (rats---wheel exercise).might noticed code little difference approach, good! ’re going adjust code slightly, can similar resampling exercise using linear model. look , ’re using data set penguins.tidyverseThe penguins data set part library called palmerpenguins, ’ll install load:Let’s attach data remove missing values. ’re also filter data one type penguin, just make analysis bit easier follow.can see 8 variables. ’ll come back later sessions, now ’re focussing 3:species type penguinflipper_length_mm length flipper mmbill_length_mm length bill mmTo practise, ’ll look relationship flipper length bill length, comparing two species selected.tidyverseLooking data, seems overall positive relationship flipper length bill length. relationship species-dependent, doesn’t seem much interaction going , since lines best fit pretty much parallel.Let’s look models resampling perspective.tidyverseFirst, specify model. ’re creating additive model, bill_length_mm depends flipper_length_mm species:aside, Power analysis session Core statistics looked model evaluation. something similar see interaction flipper_length_mm species:can see AIC value gets tiny bit worse drop interaction term. means species contributing model, although tiny bit.Next, fit models resamples data set:Lastly, can get p-value, comparing likely observed_fit based resampled fits simulated:, ’re likely get warning stating result approximation based number reps chosen. ’s unlikely true p-value zero.based , seems unlikely ’d get coefficients linear model data described best horizontal line (pretty obvious looking data!).Alternatively, can view plotting simulated null distributions placing coefficient values observed linear model top:can also compare looking confidence intervals simulated coefficients. ’re showing 95% confidence intervals, comparing observed values coefficients (.e. ones get fitting model actual data)., interpret results? Well, coefficients linear model fitted actual data miles away coefficients obtained simulated data. Remember, simulated data permuted (randomly shuffled) bill_length_mm values, refitted linear model calculated corresponding coefficients.fine relationship bill length, flipper length species. reshuffling data effect. clearly relationship variables, can see plotted data line best fit.Just satisfy curiosity (’re still point surely must curious!), can check normal approach, fit linear model perform ANOVA:Note ’ve included interaction flipper length species (flipper_length_mm:species) , consistent AIC result, ’s much data suggest interaction two variables.Finally, ANOVA confirms ’re seeing permutation test: data described best horizontal line, linear model able account good proportion variance data (adjusted R-squared value 0.37).","code":"\ninstall.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\npenguins <- penguins %>%\n  filter(species != \"Adelie\") %>% \n  drop_na()\n\npenguins## # A tibble: 187 × 7\n##    species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n##    <fct>   <fct>           <dbl>         <dbl>             <int>       <int>\n##  1 Gentoo  Biscoe           46.1          13.2               211        4500\n##  2 Gentoo  Biscoe           50            16.3               230        5700\n##  3 Gentoo  Biscoe           48.7          14.1               210        4450\n##  4 Gentoo  Biscoe           50            15.2               218        5700\n##  5 Gentoo  Biscoe           47.6          14.5               215        5400\n##  6 Gentoo  Biscoe           46.5          13.5               210        4550\n##  7 Gentoo  Biscoe           45.4          14.6               211        4800\n##  8 Gentoo  Biscoe           46.7          15.3               219        5200\n##  9 Gentoo  Biscoe           43.3          13.4               209        4400\n## 10 Gentoo  Biscoe           46.8          15.4               215        5150\n## # … with 177 more rows, and 1 more variable: sex <fct>\nggplot(penguins, aes(x = flipper_length_mm,\n                     y = bill_length_mm,\n                     colour = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\nobserved_fit <- penguins %>% \n  specify(bill_length_mm ~ flipper_length_mm + species) %>% \n  fit()\n# define the model\nlm_penguins <- lm(bill_length_mm ~ flipper_length_mm * species,\n                  data = penguins)\n\n# have a look at the coefficients\nsummary(lm_penguins)## \n## Call:\n## lm(formula = bill_length_mm ~ flipper_length_mm * species, data = penguins)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6.6977 -1.6578 -0.0014  1.4064 12.4394 \n## \n## Coefficients:\n##                                  Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)                       5.59338    8.65665   0.646   0.5190    \n## flipper_length_mm                 0.22081    0.04418   4.998 1.35e-06 ***\n## speciesGentoo                   -26.08126   11.67592  -2.234   0.0267 *  \n## flipper_length_mm:speciesGentoo   0.09247    0.05702   1.622   0.1066    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.579 on 183 degrees of freedom\n## Multiple R-squared:  0.3774, Adjusted R-squared:  0.3672 \n## F-statistic: 36.97 on 3 and 183 DF,  p-value: < 2.2e-16\n# do a backwards stepwise elimination\nstats::step(lm_penguins)## Start:  AIC=358.28\n## bill_length_mm ~ flipper_length_mm * species\n## \n##                             Df Sum of Sq    RSS    AIC\n## <none>                                   1217.1 358.28\n## - flipper_length_mm:species  1    17.491 1234.6 358.95## \n## Call:\n## lm(formula = bill_length_mm ~ flipper_length_mm * species, data = penguins)\n## \n## Coefficients:\n##                     (Intercept)                flipper_length_mm  \n##                         5.59338                          0.22081  \n##                   speciesGentoo  flipper_length_mm:speciesGentoo  \n##                       -26.08126                          0.09247\npenguins_resample <- penguins %>% \n  specify(bill_length_mm ~ flipper_length_mm + species) %>% \n  hypothesise(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  fit()\nget_p_value(penguins_resample, obs_stat = observed_fit, direction = \"two-sided\")## # A tibble: 3 × 2\n##   term              p_value\n##   <chr>               <dbl>\n## 1 flipper_length_mm       0\n## 2 intercept               0\n## 3 speciesGentoo           0\npenguins_resample %>% \n  visualise() +\n  shade_p_value(obs_stat = observed_fit, direction = \"two-sided\")\n# generate the 95% confidence intervals\nget_confidence_interval(\n  penguins_resample, \n  point_estimate = observed_fit, \n  level = .95\n)## # A tibble: 3 × 3\n##   term              lower_ci upper_ci\n##   <chr>                <dbl>    <dbl>\n## 1 flipper_length_mm  -0.0703   0.0753\n## 2 intercept          33.1     61.7   \n## 3 speciesGentoo      -1.91     1.78\n# display the coefficients of the observed linear model\nobserved_fit## # A tibble: 3 × 2\n##   term              estimate\n##   <chr>                <dbl>\n## 1 intercept           -5.28 \n## 2 flipper_length_mm    0.276\n## 3 speciesGentoo       -7.18\n# fit the model\nlm_penguins <- lm(bill_length_mm ~ flipper_length_mm * species,\n                  data = penguins)\n\n# check assumptions (which all look fine)\nlibrary(ggResidpanel)\nlm_penguins %>% \n  resid_panel(c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\nanova(lm_penguins)## Analysis of Variance Table\n## \n## Response: bill_length_mm\n##                            Df  Sum Sq Mean Sq  F value    Pr(>F)    \n## flipper_length_mm           1   49.33   49.33   7.4174  0.007085 ** \n## species                     1  670.92  670.92 100.8749 < 2.2e-16 ***\n## flipper_length_mm:species   1   17.49   17.49   2.6298  0.106594    \n## Residuals                 183 1217.14    6.65                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"single-predictor-permutation-tests.html","id":"key-points-2","chapter":"6 Single predictor permutation tests","heading":"6.8 Key points","text":"Permutation techniques applicable regardless underlying distributionThey allow test non-standard metricsThey require sufficient dataWe can use workflow infer package, part tidymodels perform permutations dataWe specify() model, use hypothesise() define null hypothesis, generate() reshuffled data calculate() statistic interestWe can reiterate workflow obtain distribution p-values","code":""}]
