[{},{"path":"index.html","id":"overview","chapter":"1 Overview","heading":"1 Overview","text":"sessions intended enable perform additional data analysis techniques appropriately confidently using R Python.Ongoing formative assessment exercisesOngoing formative assessment exercisesNo formal assessmentNo formal assessmentNo mathematical derivationsNo mathematical derivationsNo pen paper calculationsNo pen paper calculationsThey “mindlessly use stats program” course!","code":""},{"path":"index.html","id":"core-aims","chapter":"1 Overview","heading":"1.1 Core aims","text":"know presented non-standard dataset e.g.Know deal non-normal dataKnow analyse count dataBe able deal random effects","code":""},{"path":"index.html","id":"core-topics","chapter":"1 Overview","heading":"1.2 Core topics","text":"Generalised linear models","code":""},{"path":"index.html","id":"index-datasets","chapter":"1 Overview","heading":"1.3 Datasets","text":"course uses various data sets. easiest way accessing creating R-project RStudio. download data folder right-clicking link Save …. Next unzip file copy working directory. data accessible via <working-directory-name>/data..panelset{--panel-tab-font-family: inherit;}","code":""},{},{"path":"principal-component-analysis-pca.html","id":"principal-component-analysis-pca","chapter":"2 Principal component analysis (PCA)","heading":"2 Principal component analysis (PCA)","text":"","code":""},{"path":"principal-component-analysis-pca.html","id":"objectives","chapter":"2 Principal component analysis (PCA)","heading":"2.1 Objectives","text":"QuestionsHow ……ObjectivesBe able …Use…","code":""},{"path":"principal-component-analysis-pca.html","id":"purpose-and-aim","chapter":"2 Principal component analysis (PCA)","heading":"2.2 Purpose and aim","text":"statistical technique reducing dimensionality data set. technique aims find new set variables describing data. new variables made weighted sum old variables. weighting chosen new variables can ranked terms importance first new variable chosen account much variation data possible. second new variable chosen account much remaining variation data possible, many new variables old variables.","code":""},{"path":"principal-component-analysis-pca.html","id":"libraries-and-functions","chapter":"2 Principal component analysis (PCA)","heading":"2.3 Libraries and functions","text":"tidyverse","code":""},{"path":"principal-component-analysis-pca.html","id":"data","chapter":"2 Principal component analysis (PCA)","heading":"2.4 Data","text":"First need data! liven things bit, ’ll using data palmerpenguins package. package whole bunch data penguins. ’s love?Penguins\npenguins data set comes palmerpenguins package (information, see GitHub page).","code":""},{"path":"principal-component-analysis-pca.html","id":"visualise-the-data","chapter":"2 Principal component analysis (PCA)","heading":"2.5 Visualise the data","text":"First , let’s look data. always good idea get sense data.tidyverse\nFirst, load inspect data:can see different kinds variables, factors numerical. Also, appear missing data data set, probably deal .Lastly, careful year column: recognised numerical column (contains numbers), view factor, since years categorical meaning.get better sense data plot numerical variables , see possible correlation . However, quite , might easier just create correlation matrix.tidyverse\nFirst, load corrr package, allows us plot correlation matrix using tidyverse syntax:get message (error) correlation method used pearson, default. also get message missing values treated, complete pairwise comparisons made.can see , example, strong positive correlation flipper_length_mm body_mass_g. variable combinations seem reasonably well-correlated, flipper_length_mm bill_length_mm (positive) flipper_length_mm bill_depth_mm (negative).many different variables appear correlated , just lots variables data don’t know look, can useful reduce number variables. can dimension reduction methods, Principal Component Analysis (PCA) one.Basically, PCA replaces original variables new ones: principal components. principal components consist parts original variables.compare smoothy consisting , let’s say, 80% orange, 10% strawberry 10% banana (kale).Similarly, new principal component consist 80% flipper_length_mm, 10% body_mass_g 10% bill_depth_mm (still kale).tidyverse\nperform PCA, ’ll use recipe() pre-processing steps:remove NA valuescentre predictorsscale predictors","code":"\n# attach the data\ndata(package = 'palmerpenguins')\n\n# inspect the data\npenguins## # A tibble: 344 × 8\n##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n##    <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n##  1 Adelie  Torgersen           39.1          18.7               181        3750\n##  2 Adelie  Torgersen           39.5          17.4               186        3800\n##  3 Adelie  Torgersen           40.3          18                 195        3250\n##  4 Adelie  Torgersen           NA            NA                  NA          NA\n##  5 Adelie  Torgersen           36.7          19.3               193        3450\n##  6 Adelie  Torgersen           39.3          20.6               190        3650\n##  7 Adelie  Torgersen           38.9          17.8               181        3625\n##  8 Adelie  Torgersen           39.2          19.6               195        4675\n##  9 Adelie  Torgersen           34.1          18.1               193        3475\n## 10 Adelie  Torgersen           42            20.2               190        4250\n## # … with 334 more rows, and 2 more variables: sex <fct>, year <int>\nlibrary(corrr)\n\npenguins_corr <- penguins %>%\n  select(where(is.numeric)) %>%  # select the numerical columns\n  correlate() %>%                # calculate the correlations\n  rearrange()                    # rearrange highly correlated## \n## Correlation method: 'pearson'\n## Missing treated using: 'pairwise.complete.obs'\n                                 # variables close together\n\npenguins_corr## # A tibble: 5 × 6\n##   term         flipper_length_… body_mass_g bill_length_mm    year bill_depth_mm\n##   <chr>                   <dbl>       <dbl>          <dbl>   <dbl>         <dbl>\n## 1 flipper_len…           NA          0.871          0.656   0.170        -0.584 \n## 2 body_mass_g             0.871     NA              0.595   0.0422       -0.472 \n## 3 bill_length…            0.656      0.595         NA       0.0545       -0.235 \n## 4 year                    0.170      0.0422         0.0545 NA            -0.0604\n## 5 bill_depth_…           -0.584     -0.472         -0.235  -0.0604       NA\npenguin_recipe <-\n  recipe(~., data = penguins) %>% \n  update_role(species, island, sex, year, new_role = \"id\") %>% \n  step_naomit(all_predictors()) %>% \n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors(), id = \"pca\") %>% \n  prep()\n\npenguin_pca <- \n  penguin_recipe %>% \n  tidy(id = \"pca\") \n\npenguin_pca## # A tibble: 16 × 4\n##    terms                value component id   \n##    <chr>                <dbl> <chr>     <chr>\n##  1 bill_length_mm     0.455   PC1       pca  \n##  2 bill_depth_mm     -0.400   PC1       pca  \n##  3 flipper_length_mm  0.576   PC1       pca  \n##  4 body_mass_g        0.548   PC1       pca  \n##  5 bill_length_mm    -0.597   PC2       pca  \n##  6 bill_depth_mm     -0.798   PC2       pca  \n##  7 flipper_length_mm -0.00228 PC2       pca  \n##  8 body_mass_g       -0.0844  PC2       pca  \n##  9 bill_length_mm    -0.644   PC3       pca  \n## 10 bill_depth_mm      0.418   PC3       pca  \n## 11 flipper_length_mm  0.232   PC3       pca  \n## 12 body_mass_g        0.597   PC3       pca  \n## 13 bill_length_mm     0.146   PC4       pca  \n## 14 bill_depth_mm     -0.168   PC4       pca  \n## 15 flipper_length_mm -0.784   PC4       pca  \n## 16 body_mass_g        0.580   PC4       pca"},{"path":"principal-component-analysis-pca.html","id":"visualising-pcs","chapter":"2 Principal component analysis (PCA)","heading":"2.5.1 Visualising PCs","text":"Now ’ve performed PCA, can bit closer look. useful way looking much PCs (principal components) contributing amount variance explained create screeplot. Basically, plots percentage explained variance PC.tidyverse\ncan extract relevant data directly penguin_recipe object:definition, first principal component (PC1) always explain largest amount variation. case, PC1 explains almost 70% variance!’s pretty good going, since means instead look four variables, look just one still capture 70% data. number variables data set manageable, probably wouldn’t . However, data set hundreds variables, seeing can described well using PCs useful thing .","code":"\npenguin_recipe %>% \n  tidy(id = \"pca\", type = \"variance\") %>% \n  filter(terms == \"percent variance\") %>% \n  ggplot(aes(x = component, y = value)) + \n  geom_col() + \n  xlim(c(0, 5)) + \n  ylab(\"% of total variance\")"},{"path":"principal-component-analysis-pca.html","id":"loadings","chapter":"2 Principal component analysis (PCA)","heading":"2.5.2 Loadings","text":"","code":"\n# get pca loadings into wider format\npca_wider <- penguin_pca %>% \n  tidyr::pivot_wider(names_from = component, id_cols = terms)\n\n# define arrow style\narrow_style <- arrow(length = unit(.05, \"inches\"),\n                     type = \"closed\")\n\n\npca_plot <-\n  juice(penguin_recipe) %>%\n  ggplot(aes(PC1, PC2)) +\n  geom_point(aes(color = species, shape = species), \n             alpha = 0.8, \n             size = 2)\n\npca_plot +\n  geom_segment(data = pca_wider,\n               aes(xend = PC1, yend = PC2), \n               x = 0, \n               y = 0, \n               arrow = arrow_style) + \n  geom_text(data = pca_wider,\n            aes(x = PC1, y = PC2, label = terms), \n            hjust = 0, \n            vjust = 1,\n            size = 5, \n            color = '#0A537D') \npenguin_pca %>%\n  mutate(terms = reorder_within(terms, \n                                abs(value), \n                                component)) %>%\n  ggplot(aes(abs(value), terms, fill = value > 0)) +\n  geom_col() +\n  facet_wrap(~component, scales = \"free_y\") +\n  tidytext::scale_y_reordered() +\n  labs(\n    x = \"Absolute value of contribution\",\n    y = NULL, fill = \"Positive?\"\n  ) "},{"path":"principal-component-analysis-pca.html","id":"exercise","chapter":"2 Principal component analysis (PCA)","heading":"2.6 Exercise","text":"Exercise 2.1  Exercisetidyversequestiontidyverseanswer","code":""},{"path":"principal-component-analysis-pca.html","id":"key-points","chapter":"2 Principal component analysis (PCA)","heading":"2.7 Key points","text":"Point 1Point 2Point 3.panelset{--panel-tab-font-family: inherit;}","code":""},{},{"path":"hierarchical-clustering.html","id":"hierarchical-clustering","chapter":"3 Hierarchical clustering","heading":"3 Hierarchical clustering","text":"","code":""},{"path":"hierarchical-clustering.html","id":"objectives-1","chapter":"3 Hierarchical clustering","heading":"3.1 Objectives","text":"QuestionsHow ……ObjectivesBe able …Use…","code":""},{"path":"hierarchical-clustering.html","id":"purpose-and-aim-1","chapter":"3 Hierarchical clustering","heading":"3.2 Purpose and aim","text":"Hierarchical clustering form cluster analysis, aim create hierarchy clusters. results commonly displayed dendogram displays clusters found analysis.","code":""},{"path":"hierarchical-clustering.html","id":"libraries-and-functions-1","chapter":"3 Hierarchical clustering","heading":"3.3 Libraries and functions","text":"tidyversePlot dendrogram using ggdendrogram().Cutting tree:Plot gene expression trends, separating genes interest cluster:can update , adding median line facet, showing median expression cluster:","code":"\ntrans_cts <- read_csv(\"data/transcriptome/counts_transformed.csv\")\nsample_info <- read_csv(\"data/transcriptome/sample_info.csv\")\ntest_result <- read_csv(\"data/transcriptome/test_result.csv\")\n\n# set of candidate genes for clustering\n# we select the 50 genes with the lowest padj value\ncandidate_genes <- test_result %>% \n  slice_min(padj, n = 50) %>%  \n  select(gene) %>% \n  distinct()\n\n# Summarise counts \ntrans_cts_mean <- trans_cts %>% \n  # convert to long format\n  pivot_longer(cols = wt_0_r1:mut_180_r3, names_to = \"sample\", values_to = \"cts\")  %>% \n  # join with sample info table\n  full_join(sample_info, by = \"sample\") %>% \n  # filter to retain only genes of interest\n  semi_join(candidate_genes, by = \"gene\") %>%\n  # for each gene\n  group_by(gene) %>% \n  # scale the cts column\n  mutate(cts_scaled = (cts - mean(cts))/sd(cts)) %>% \n  # for each gene, strain and minute\n  group_by(gene, strain, minute) %>%\n  # calculate the mean (scaled) cts\n  summarise(mean_cts_scaled = mean(cts_scaled),\n            nrep = n()) %>% \n  ungroup()\n\ngene_hclust <- trans_cts %>%\n  semi_join(candidate_genes, by = \"gene\") %>%\n  column_to_rownames(var = \"gene\") %>% \n  dist(method = \"euclidian\") %>% \n  hclust()\nggdendrogram(gene_hclust, rotate = FALSE, size = 3)\nggdendrogram(gene_hclust, rotate = FALSE, size = 3) +\n  geom_hline(yintercept = 22, colour = \"red\")\ngene_cluster <- cutree(gene_hclust, k = 4) %>% \n  # turn the named vector into a tibble\n  enframe() %>% \n  # rename some of the columns\n  rename(gene = name, cluster = value)\n\nhead(gene_cluster)## # A tibble: 6 × 2\n##   gene          cluster\n##   <chr>           <int>\n## 1 SPAC22A12.17c       1\n## 2 SPACUNK4.17         1\n## 3 SPAC4H3.03c         2\n## 4 SPAC2F3.05c         3\n## 5 SPAC637.03          2\n## 6 SPAC26F1.07         4\ntrans_cts_cluster <- trans_cts_mean %>% \n  inner_join(gene_cluster, by = \"gene\")\n\nhead(trans_cts_cluster)## # A tibble: 6 × 6\n##   gene        strain minute mean_cts_scaled  nrep cluster\n##   <chr>       <chr>   <dbl>           <dbl> <int>   <int>\n## 1 SPAC11E3.14 mut         0          -1.06      3       3\n## 2 SPAC11E3.14 mut        15           0.615     3       3\n## 3 SPAC11E3.14 mut        30           1.83      3       3\n## 4 SPAC11E3.14 mut        60          -0.553     3       3\n## 5 SPAC11E3.14 mut       120          -0.353     3       3\n## 6 SPAC11E3.14 mut       180          -0.635     3       3\ntrans_cts_cluster %>% \n  ggplot(aes(minute, mean_cts_scaled)) +\n  geom_line(aes(group = gene)) +\n  facet_grid(rows = vars(strain), cols = vars(cluster))\ntrans_cts_cluster %>% \n  ggplot(aes(minute, mean_cts_scaled)) +\n  geom_line(aes(group = gene), alpha = 0.3) +\n  geom_line(stat = \"summary\", fun = \"median\", colour = \"red\", size = 0.5, \n            aes(group = 1)) +\n  facet_grid(rows = vars(strain), cols = vars(cluster))"},{"path":"hierarchical-clustering.html","id":"advanced-colouring-clusters","chapter":"3 Hierarchical clustering","heading":"3.4 Advanced: Colouring clusters","text":"Adding colour dendograms can bit tricky. different ways , simplest method ’ve found (far) require installation various, extensive packages bunch functions written Atrebas. See blogpost.Basically, need load script provided go!","code":"\nsource(file = \"scripts/ggdendro_extended.R\")\n# cut the dendrogram, specify the number of clusters\nhcdata <- dendro_data_k(gene_hclust, 4)\n\n\nplot_ggdendro(hcdata,\n              direction   = \"lr\",\n              expand.y    = 0.2,\n              branch.size = 0.5)\nplot_ggdendro(hcdata,\n              fan         = TRUE,\n              label.size  = 3,\n              nudge.label = 0.02,\n              expand.y    = 0.4) +\n  theme_void()"},{"path":"hierarchical-clustering.html","id":"exercise-1","chapter":"3 Hierarchical clustering","heading":"3.5 Exercise","text":"Exercise 3.1  Exercisetidyversequestiontidyverseanswer","code":""},{"path":"hierarchical-clustering.html","id":"key-points-1","chapter":"3 Hierarchical clustering","heading":"3.6 Key points","text":"Point 1Point 2Point 3","code":""}]
