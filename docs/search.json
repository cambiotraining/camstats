[{"path":"index.html","id":"overview","chapter":"1 Overview","heading":"1 Overview","text":"sessions intended enable perform additional data analysis techniques appropriately confidently using R Python.Ongoing formative assessment exercisesOngoing formative assessment exercisesNo formal assessmentNo formal assessmentNo mathematical derivationsNo mathematical derivationsNo pen paper calculationsNo pen paper calculationsThey “mindlessly use stats program” course!","code":""},{"path":"index.html","id":"core-aims","chapter":"1 Overview","heading":"1.1 Core aims","text":"know presented non-standard dataset e.g.Know deal non-normal dataKnow analyse count dataBe able deal random effects","code":""},{"path":"index.html","id":"core-topics","chapter":"1 Overview","heading":"1.2 Core topics","text":"Generalised linear models","code":""},{"path":"index.html","id":"index-datasets","chapter":"1 Overview","heading":"1.3 Datasets","text":"course uses various data sets. easiest way accessing creating R-project RStudio. download data folder right-clicking link Save …. Next unzip file copy working directory. data accessible via <working-directory-name>/data..panelset{--panel-tab-font-family: inherit;}","code":"## Warning: 'xaringanExtra::style_panelset' is deprecated.\n## Use 'style_panelset_tabs' instead.\n## See help(\"Deprecated\")"},{"path":"kmeans.html","id":"kmeans","chapter":"2 K-means clustering","heading":"2 K-means clustering","text":"","code":""},{"path":"kmeans.html","id":"objectives","chapter":"2 K-means clustering","heading":"2.1 Objectives","text":"Understand k-means clustering worksBe able perform k-means clusteringBe able optimise cluster number","code":""},{"path":"kmeans.html","id":"libraries-and-functions","chapter":"2 K-means clustering","heading":"2.2 Libraries and functions","text":"tidyversebase R","code":""},{"path":"kmeans.html","id":"workflow","chapter":"2 K-means clustering","heading":"2.3 Workflow","text":"K-means clustering iterative process. follows following steps:Select number clusters identify (e.g. K = 3)Create centroidsPlace centroids randomly dataAssign data point closest centroidCalculate centroid new clusterRepeat steps 4-5 clusters change","code":""},{"path":"kmeans.html","id":"datasets","chapter":"2 K-means clustering","heading":"2.4 Datasets","text":"First need data! liven things bit, ’ll using data palmerpenguins package. package whole bunch data penguins. ’s love?DataPenguins\npenguins data set comes palmerpenguins package (information, see GitHub page).Darwin’s finches\nfinches dataset adapted accompanying website 40 years evolution. Darwin’s finches Daphne Major Island Peter R. Grant Rosemary B. Grant.really interesting lecture findings Grants can found (1h10min).","code":"\n# attach the data\ndata(package = 'palmerpenguins')\n# load the data\nfinches <- read_csv(\"data/finch_beaks.csv\")"},{"path":"kmeans.html","id":"visualise-the-data","chapter":"2 K-means clustering","heading":"2.5 Visualise the data","text":"First , let’s look data. always good idea get sense data.tidyversebase RPythonSo different types penguins, different islands. Bill flipper measurements taken, penguins’ weight plus sex recorded.let’s look flipper length versus bill length, example.tidyversebase RWe can already see data appear cluster quite closely species. great example illustrate K-means clustering (’d almost think chose example purpose!)","code":"\npenguins## # A tibble: 344 × 8\n##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n##    <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n##  1 Adelie  Torgersen           39.1          18.7               181        3750\n##  2 Adelie  Torgersen           39.5          17.4               186        3800\n##  3 Adelie  Torgersen           40.3          18                 195        3250\n##  4 Adelie  Torgersen           NA            NA                  NA          NA\n##  5 Adelie  Torgersen           36.7          19.3               193        3450\n##  6 Adelie  Torgersen           39.3          20.6               190        3650\n##  7 Adelie  Torgersen           38.9          17.8               181        3625\n##  8 Adelie  Torgersen           39.2          19.6               195        4675\n##  9 Adelie  Torgersen           34.1          18.1               193        3475\n## 10 Adelie  Torgersen           42            20.2               190        4250\n## # … with 334 more rows, and 2 more variables: sex <fct>, year <int>\nhead(penguins)## # A tibble: 6 × 8\n##   species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  \n##   <fct>   <fct>           <dbl>         <dbl>            <int>       <int> <fct>\n## 1 Adelie  Torge…           39.1          18.7              181        3750 male \n## 2 Adelie  Torge…           39.5          17.4              186        3800 fema…\n## 3 Adelie  Torge…           40.3          18                195        3250 fema…\n## 4 Adelie  Torge…           NA            NA                 NA          NA <NA> \n## 5 Adelie  Torge…           36.7          19.3              193        3450 fema…\n## 6 Adelie  Torge…           39.3          20.6              190        3650 male \n## # … with 1 more variable: year <int>from plotnine import *\nfrom plotnine.data import *\nimport numpy as np\nimport pandas as pdpenguins = pd.read_csv(\"data/penguins.csv\")ggplot(data = penguins) +\\\ngeom_point(mapping = aes(x = \"flipper_length_mm\",\n                         y=\"bill_length_mm\",\n                         colour=\"species\"))\nggplot(penguins, aes(x = flipper_length_mm,\n                     y = bill_length_mm,\n                     colour = species)) +\n  geom_point()\nplot(penguins$flipper_length_mm,  # scatter plot\n     penguins$bill_length_mm,\n     pch = 20,\n     col = penguins$species)      # colour by species\n\nlegend(\"bottomright\",             # legend\n       legend = levels(penguins$species),\n       pch = 20,\n       col = factor(levels(penguins$species)))"},{"path":"kmeans.html","id":"exercise---bill-depth-and-length","chapter":"2 K-means clustering","heading":"2.6 Exercise - Bill depth and length","text":"Exercise 2.1  exercise ’d like create scatter plot bill depth bill length. ’ll using colour separate female/male data. Lastly, ’re creating individual panels island.code given like replace <FIXME> parts required code.think many clusters try divide data .tidyversebase R\nThings bit convoluted using base R, compared tidyverse. ’s faceting equivalent can implement directly.split data island create loop plot island. ’m loathe go , ’ll just use Biscoe island example ’m sure ’re able adapt things accordingly Dream Torgersen islands!tidyversebase R","code":"penguins %>% \n  drop_na() %>%                     \n  ggplot(aes(x = <FIXME>,\n             y = bill_length_mm,\n             colour = <FIXME>)) +\n  geom_<FIXME>() +\n  facet_wrap(facets = vars(<FIXME>))biscoe <- \n  penguins[penguins$island == \"<FIXME>\", ]\n\nplot(biscoe$<FIXME>,\n     biscoe$bill_length_mm,\n     pch = 20,\n     col = biscoe$<FIXME>)\n\nlegend(\"bottomright\",\n       legend = levels(biscoe$sex),\n       pch = 20,\n       col = factor(levels(biscoe$sex)))\n\ntitle(main = \"Biscoe\")\npenguins %>% \n  drop_na() %>%                     \n  ggplot(aes(x = bill_depth_mm,\n             y = bill_length_mm,\n             colour = sex)) +\n  geom_point() +\n  facet_wrap(facets = vars(island))\nbiscoe <- \n  penguins[penguins$island == \"Biscoe\", ]\n\nplot(biscoe$bill_depth_mm,\n     biscoe$bill_length_mm,\n     pch = 20,\n     col = biscoe$sex)\n\nlegend(\"bottomright\",\n       legend = levels(biscoe$sex),\n       pch = 20,\n       col = factor(levels(biscoe$sex)))\n\ntitle(main = \"Biscoe\")"},{"path":"kmeans.html","id":"clustering","chapter":"2 K-means clustering","heading":"2.7 Clustering","text":"Next, ’ll actual clustering.tidyverse\nclustering, ’ll using kmeans() function. function requires numeric data input.Note output list vectors, differing lengths. ’s contain different types information:cluster contains information pointcenters, withinss, size contain information clustertotss, tot.withinss, betweenss, iter contain information full clusteringbase R\nclustering, ’ll using kmeans() function. function requires numeric data input.Note output list vectors, differing lengths. ’s contain different types information:cluster contains information pointcenters, withinss, size contain information clustertotss, tot.withinss, betweenss, iter contain information full clustering","code":"\npoints <-\n  penguins %>% \n  select(flipper_length_mm,      # select data\n         bill_length_mm) %>% \n  drop_na()                      # remove missing values\n\nkclust <-\n  kmeans(points,                 # perform k-means clustering\n         centers = 3)            # using 3 centers\n\nsummary(kclust)                  # summarise output##              Length Class  Mode   \n## cluster      342    -none- numeric\n## centers        6    -none- numeric\n## totss          1    -none- numeric\n## withinss       3    -none- numeric\n## tot.withinss   1    -none- numeric\n## betweenss      1    -none- numeric\n## size           3    -none- numeric\n## iter           1    -none- numeric\n## ifault         1    -none- numeric\npoints_r <-\n  data.frame(\n    penguins$flipper_length_mm,  # get numeric data\n    penguins$bill_length_mm) |>  # use base R pipe!\n  na.omit()                      # remove missing data\n\nkclust_r <-\n  kmeans(points_r,               # perform k-means clustering\n         centers = 3)            # using 3 centers\n\nsummary(kclust_r)                # summarise output##              Length Class  Mode   \n## cluster      342    -none- numeric\n## centers        6    -none- numeric\n## totss          1    -none- numeric\n## withinss       3    -none- numeric\n## tot.withinss   1    -none- numeric\n## betweenss      1    -none- numeric\n## size           3    -none- numeric\n## iter           1    -none- numeric\n## ifault         1    -none- numeric"},{"path":"kmeans.html","id":"visualise-clusters","chapter":"2 K-means clustering","heading":"2.8 Visualise clusters","text":"can visualise clusters calculated .tidyverse\nperformed clustering, centers calculated. values give (x, y) coordinates centroids.initial centroids get randomly placed data. , combined iterative nature process, means values see going slightly different values . ’s normal!Next, want visualise data points belong cluster. can follows:base R\nperformed clustering, centers calculated. values give (x, y) coordinates centroids.initial centroids get randomly placed data. , combined iterative nature process, means values see going slightly different values . ’s normal!Next, want visualise data points belong cluster. can follows:","code":"\ntidy_clust <- tidy(kclust) # get centroid coordinates\n\ntidy_clust## # A tibble: 3 × 5\n##   flipper_length_mm bill_length_mm  size withinss cluster\n##               <dbl>          <dbl> <int>    <dbl> <fct>  \n## 1              197.           46.0    93    3932. 1      \n## 2              187.           38.4   120    3494. 2      \n## 3              217.           47.6   129    6658. 3\nkclust %>%                              # take clustering data\n  augment(points) %>%                   # combine with original data\n  ggplot(aes(x = flipper_length_mm,     # plot the original data\n             y = bill_length_mm)) +\n  geom_point(aes(colour = .cluster)) +  # colour by classification\n  geom_point(data = tidy_clust,\n             size = 7, shape = \"x\")     # add the cluster centers\nkclust_r$centers  # get centroid coordinates##   penguins.flipper_length_mm penguins.bill_length_mm\n## 1                   216.8837                47.56744\n## 2                   196.7312                45.95484\n## 3                   186.9917                38.42750\nplot(points_r,                # plot original data\n     col = kclust_r$cluster,  # colour by cluster\n     pch = 20)\n\npoints(kclust_r$centers,      # add cluster centers\n       pch = 4,\n       lwd = 3)"},{"path":"kmeans.html","id":"optimising-cluster-number","chapter":"2 K-means clustering","heading":"2.9 Optimising cluster number","text":"example set number clusters 3. made sense, data already visually separated roughly three groups - one species.However, might cluster number choose lot less obvious. case helpful explore clustering data range clusters.Reiterating range k values reasonably straightforward using tidyverse. short, determine values k want explore loop values, repeating workflow looked previously.tidyverse\nAlthough write function loop k values, tidyverse series map() functions can . information .short, map() function spits list contains output. data, can create table contains lists information need.calculate following:kclust column contains list kmeans() output, value kthe tidied column contains information per-cluster basisthe glanced column contains single-row summary k - ’ll use tot.withinss values little bit later onthe augmented column contains original data, augmented classification calculated kmeans() functionLists can sometimes bit tricky get head around, ’s worthwhile exploring output. RStudio particularly useful , since can just left-click object Environment panel look.way see lists context containers. one huge table kclusts contains information need. ‘cell’ table container relevant data. kclust column list kmeans objects (output kmeans() k values), whereas columns lists tibbles (tidy(), glance() augment() functions output tibble information value k).us use data lists, makes sense extract column--column basis. ’re ignoring kclust column, don’t need actual kmeans() output .extract data lists use unnest() function.Next, can visualise data. ’ll start plotting original data colouring data points based final cluster assigned kmeans() function.(augmented) data assignments. look structure table.facet data k, get single panel value k.also add calculated cluster centres, stored clusters.Looking plot shows already knew (things easy time!): three clusters pretty good choice data. Remember ’re looking clusters distinct, .e. separated one another. example, using k = 4 gives four nice groups, two directly adjacent, suggesting equally well single cluster.base R","code":"\nkclusts <- \n  tibble(k = 1:6) %>%                         # check for k = 1 to 6\n  mutate(\n    kclust = map(k, ~kmeans(points, .x)),     # perform clustering for each k\n    tidied = map(kclust, tidy),               # summary at per-cluster level\n    glanced = map(kclust, glance),            # get single-row summary\n    augmented = map(kclust, augment, points)  # add classification to data set\n  )\n\nkclusts## # A tibble: 6 × 5\n##       k kclust   tidied           glanced          augmented         \n##   <int> <list>   <list>           <list>           <list>            \n## 1     1 <kmeans> <tibble [1 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n## 2     2 <kmeans> <tibble [2 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n## 3     3 <kmeans> <tibble [3 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n## 4     4 <kmeans> <tibble [4 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n## 5     5 <kmeans> <tibble [5 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n## 6     6 <kmeans> <tibble [6 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\nclusters <- \n  kclusts %>%\n  unnest(cols = c(tidied))\n\nassignments <- \n  kclusts %>% \n  unnest(cols = c(augmented))\n\nclusterings <- \n  kclusts %>%\n  unnest(cols = c(glanced))\nggplot(assignments,\n       aes(x = flipper_length_mm,     # plot data\n           y = bill_length_mm)) +  \n  geom_point(aes(color = .cluster),   # colour by cluster\n             alpha = 0.8) +           # add transparency\n  facet_wrap(~ k) +                   # facet for each k\n  geom_point(data = clusters,         # add centers\n             size = 7,\n             shape = \"x\")\n#baseR explore"},{"path":"kmeans.html","id":"elbow-plot","chapter":"2 K-means clustering","heading":"2.9.1 Elbow plot","text":"Visualising data like can helpful time can also bit subjective (hoorah!). find another subjective way interpreting clusters (remember, statistics isn’t YES/magic mushroom comfortable wandering around murky grey areas statistics now), can plot total within-cluster variation value k.Intuitively, keep adding clusters total amount variation can explained clusters increase. extreme case data point cluster can explain variation data.course sensible approach - hence us balancing number clusters much variation can capture.practical approach creating “elbow” plot cumulative amount variation explained plotted number clusters.tidyverse\noutput kmeans() function includes tot.withinss - total within-cluster sum squares.base RWe can see total within-cluster sum squares decreases number clusters increases. can also see k = 3 onwards slope line becomes much shallower. “elbow” bending point useful gauge find optimum number clusters.exploration can see three clusters optimal scenario.","code":"\nggplot(clusterings,\n       aes(x = k,                # for each k plot...\n           y = tot.withinss)) +  # total within variance\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(\n    breaks = seq(1, 6, 1))       # set the x-axis breaks"},{"path":"kmeans.html","id":"exercise","chapter":"2 K-means clustering","heading":"2.10 Exercise","text":"Exercise 2.2  Practice running clustering workflow using finches dataset. Try following:Read dataExplore visualise dataPerform clustering k = 2Find using k = 2 reasonable choiceTry draw conclusionstidyverseLet’s look data.Next, perform clustering.looks like two clusters reasonable choice. let’s explore bit .Extract relevant data.Visualise result.Create elbow plot closer look.initial clustering done using two clusters, basically capturing two different finch species.Redoing analysis different numbers clusters seems reasonably support decision. elbow plot suggests k = 3 terrible idea either.example used data collected two different time points: 1975 2012.analysis ’ve kept data together. However, original premises data see indication evolution going species finches. Think approach question!base R\nadded.","code":"\nfinches <- read_csv(\"data/finch_beaks.csv\")\nhead(finches)## # A tibble: 6 × 5\n##    band species beak_length_mm beak_depth_mm  year\n##   <dbl> <chr>            <dbl>         <dbl> <dbl>\n## 1     2 fortis             9.4           8    1975\n## 2     9 fortis             9.2           8.3  1975\n## 3    12 fortis             9.5           7.5  1975\n## 4    15 fortis             9.5           8    1975\n## 5   305 fortis            11.5           9.9  1975\n## 6   307 fortis            11.1           8.6  1975\nggplot(finches, aes(x = beak_depth_mm,\n                     y = beak_length_mm,\n                     colour = species)) +\n  geom_point()\npoints <-\n  finches %>% \n  select(beak_depth_mm,         # select data\n         beak_length_mm) %>% \n  drop_na()                      # remove missing values\n\nkclust <-\n  kmeans(points,                 # perform k-means clustering\n         centers = 2)            # using 2 centers\n\nsummary(kclust)                  # summarise output##              Length Class  Mode   \n## cluster      651    -none- numeric\n## centers        4    -none- numeric\n## totss          1    -none- numeric\n## withinss       2    -none- numeric\n## tot.withinss   1    -none- numeric\n## betweenss      1    -none- numeric\n## size           2    -none- numeric\n## iter           1    -none- numeric\n## ifault         1    -none- numeric\ntidy_clust <- tidy(kclust) # get centroid coordinates\n\ntidy_clust## # A tibble: 2 × 5\n##   beak_depth_mm beak_length_mm  size withinss cluster\n##           <dbl>          <dbl> <int>    <dbl> <fct>  \n## 1          8.98           10.5   431     442. 1      \n## 2          9.16           13.7   220     237. 2\nkclust %>%                              # take clustering data\n  augment(points) %>%                   # combine with original data\n  ggplot(aes(x = beak_depth_mm,     # plot the original data\n             y = beak_length_mm)) +\n  geom_point(aes(colour = .cluster)) +  # colour by classification\n  geom_point(data = tidy_clust,\n             size = 7, shape = \"x\")     # add the cluster centers\nkclusts <- \n  tibble(k = 1:6) %>%                         # check for k = 1 to 6\n  mutate(\n    kclust = map(k, ~kmeans(points, .x)),     # perform clustering for each k\n    tidied = map(kclust, tidy),               # summary at per-cluster level\n    glanced = map(kclust, glance),            # get single-row summary\n    augmented = map(kclust, augment, points)  # add classification to data set\n  )\nclusters <- \n  kclusts %>%\n  unnest(cols = c(tidied))\n\nassignments <- \n  kclusts %>% \n  unnest(cols = c(augmented))\n\nclusterings <- \n  kclusts %>%\n  unnest(cols = c(glanced))\nggplot(assignments,\n       aes(x = beak_depth_mm,        # plot data\n           y = beak_length_mm)) +  \n  geom_point(aes(color = .cluster),   # colour by cluster\n             alpha = 0.8) +           # add transparency\n  facet_wrap(~ k) +                   # facet for each k\n  geom_point(data = clusters,         # add centers\n             size = 7,\n             shape = \"x\")\nggplot(clusterings,\n       aes(x = k,                # for each k plot...\n           y = tot.withinss)) +  # total within variance\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(\n    breaks = seq(1, 6, 1))       # set the x-axis breaks"},{"path":"kmeans.html","id":"load-the-data","chapter":"2 K-means clustering","heading":"Load the data","text":"","code":"\nfinches <- read_csv(\"data/finch_beaks.csv\")\nhead(finches)## # A tibble: 6 × 5\n##    band species beak_length_mm beak_depth_mm  year\n##   <dbl> <chr>            <dbl>         <dbl> <dbl>\n## 1     2 fortis             9.4           8    1975\n## 2     9 fortis             9.2           8.3  1975\n## 3    12 fortis             9.5           7.5  1975\n## 4    15 fortis             9.5           8    1975\n## 5   305 fortis            11.5           9.9  1975\n## 6   307 fortis            11.1           8.6  1975"},{"path":"kmeans.html","id":"visualise-the-data-1","chapter":"2 K-means clustering","heading":"Visualise the data","text":"Let’s look data.","code":"\nggplot(finches, aes(x = beak_depth_mm,\n                     y = beak_length_mm,\n                     colour = species)) +\n  geom_point()"},{"path":"kmeans.html","id":"clustering-1","chapter":"2 K-means clustering","heading":"Clustering","text":"Next, perform clustering.","code":"\npoints <-\n  finches %>% \n  select(beak_depth_mm,         # select data\n         beak_length_mm) %>% \n  drop_na()                      # remove missing values\n\nkclust <-\n  kmeans(points,                 # perform k-means clustering\n         centers = 2)            # using 2 centers\n\nsummary(kclust)                  # summarise output##              Length Class  Mode   \n## cluster      651    -none- numeric\n## centers        4    -none- numeric\n## totss          1    -none- numeric\n## withinss       2    -none- numeric\n## tot.withinss   1    -none- numeric\n## betweenss      1    -none- numeric\n## size           2    -none- numeric\n## iter           1    -none- numeric\n## ifault         1    -none- numeric\ntidy_clust <- tidy(kclust) # get centroid coordinates\n\ntidy_clust## # A tibble: 2 × 5\n##   beak_depth_mm beak_length_mm  size withinss cluster\n##           <dbl>          <dbl> <int>    <dbl> <fct>  \n## 1          8.98           10.5   431     442. 1      \n## 2          9.16           13.7   220     237. 2"},{"path":"kmeans.html","id":"visualise-the-clusters","chapter":"2 K-means clustering","heading":"Visualise the clusters","text":"","code":"\nkclust %>%                              # take clustering data\n  augment(points) %>%                   # combine with original data\n  ggplot(aes(x = beak_depth_mm,     # plot the original data\n             y = beak_length_mm)) +\n  geom_point(aes(colour = .cluster)) +  # colour by classification\n  geom_point(data = tidy_clust,\n             size = 7, shape = \"x\")     # add the cluster centers"},{"path":"kmeans.html","id":"optimise-clusters","chapter":"2 K-means clustering","heading":"Optimise clusters","text":"looks like two clusters reasonable choice. let’s explore bit .Extract relevant data.Visualise result.Create elbow plot closer look.","code":"\nkclusts <- \n  tibble(k = 1:6) %>%                         # check for k = 1 to 6\n  mutate(\n    kclust = map(k, ~kmeans(points, .x)),     # perform clustering for each k\n    tidied = map(kclust, tidy),               # summary at per-cluster level\n    glanced = map(kclust, glance),            # get single-row summary\n    augmented = map(kclust, augment, points)  # add classification to data set\n  )\nclusters <- \n  kclusts %>%\n  unnest(cols = c(tidied))\n\nassignments <- \n  kclusts %>% \n  unnest(cols = c(augmented))\n\nclusterings <- \n  kclusts %>%\n  unnest(cols = c(glanced))\nggplot(assignments,\n       aes(x = beak_depth_mm,        # plot data\n           y = beak_length_mm)) +  \n  geom_point(aes(color = .cluster),   # colour by cluster\n             alpha = 0.8) +           # add transparency\n  facet_wrap(~ k) +                   # facet for each k\n  geom_point(data = clusters,         # add centers\n             size = 7,\n             shape = \"x\")\nggplot(clusterings,\n       aes(x = k,                # for each k plot...\n           y = tot.withinss)) +  # total within variance\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(\n    breaks = seq(1, 6, 1))       # set the x-axis breaks"},{"path":"kmeans.html","id":"conclusions","chapter":"2 K-means clustering","heading":"Conclusions","text":"initial clustering done using two clusters, basically capturing two different finch species.Redoing analysis different numbers clusters seems reasonably support decision. elbow plot suggests k = 3 terrible idea either.","code":""},{"path":"kmeans.html","id":"food-for-thought","chapter":"2 K-means clustering","heading":"Food for thought","text":"example used data collected two different time points: 1975 2012.analysis ’ve kept data together. However, original premises data see indication evolution going species finches. Think approach question!","code":""},{"path":"kmeans.html","id":"key-points","chapter":"2 K-means clustering","heading":"2.11 Key points","text":"k-means clustering partitions data clustersthe k defines number clusterscluster centers centroids get assigned randomlyeach data point gets assigned closest centroidthe centroid new clusters gets calculated process assignment recalculation repeats cluster longer changethe optimal number clusters can determined ‘elbow’ plot","code":""}]
