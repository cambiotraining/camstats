[{},{"path":"index.html","id":"overview","chapter":"1 Overview","heading":"1 Overview","text":"sessions intended enable perform additional data analysis techniques appropriately confidently using R Python.Ongoing formative assessment exercisesOngoing formative assessment exercisesNo formal assessmentNo formal assessmentNo mathematical derivationsNo mathematical derivationsNo pen paper calculationsNo pen paper calculationsThey “mindlessly use stats program” course!","code":""},{"path":"index.html","id":"core-aims","chapter":"1 Overview","heading":"1.1 Core aims","text":"know presented non-standard dataset e.g.Know deal non-normal dataKnow analyse count dataBe able deal random effects","code":""},{"path":"index.html","id":"core-topics","chapter":"1 Overview","heading":"1.2 Core topics","text":"Generalised linear models","code":""},{"path":"index.html","id":"index-datasets","chapter":"1 Overview","heading":"1.3 Datasets","text":"course uses various data sets. easiest way accessing creating R-project RStudio. download data folder right-clicking link Save …. Next unzip file copy working directory. data accessible via <working-directory-name>/data..panelset{--panel-tab-font-family: inherit;}","code":"## Warning: 'xaringanExtra::style_panelset' is deprecated.\n## Use 'style_panelset_tabs' instead.\n## See help(\"Deprecated\")"},{},{"path":"kmeans.html","id":"kmeans","chapter":"2 K-means clustering","heading":"2 K-means clustering","text":"","code":""},{"path":"kmeans.html","id":"objectives","chapter":"2 K-means clustering","heading":"2.1 Objectives","text":"Understand k-means clustering worksBe able perform k-means clusteringBe able optimise cluster number","code":""},{"path":"kmeans.html","id":"libraries-and-functions","chapter":"2 K-means clustering","heading":"2.2 Libraries and functions","text":"tidyversebase RPython","code":"from plotnine import *\nimport pandas as pd\nfrom datar.all import *from pipda import options\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\n\noptions.assume_all_piping = True"},{"path":"kmeans.html","id":"workflow","chapter":"2 K-means clustering","heading":"2.3 Workflow","text":"K-means clustering iterative process. follows following steps:Select number clusters identify (e.g. K = 3)Create centroidsPlace centroids randomly dataAssign data point closest centroidCalculate centroid new clusterRepeat steps 4-5 clusters change","code":""},{"path":"kmeans.html","id":"datasets","chapter":"2 K-means clustering","heading":"2.4 Datasets","text":"First need data! liven things bit, ’ll using data palmerpenguins package. package whole bunch data penguins. ’s love?Penguins\npenguins data set comes palmerpenguins package (information, see GitHub page).Darwin’s finches\nfinches dataset adapted accompanying website 40 years evolution. Darwin’s finches Daphne Major Island Peter R. Grant Rosemary B. Grant.really interesting lecture findings Grants can found (1h10min).","code":"\n# load the data\nfinches <- read_csv(\"data/finch_beaks.csv\")"},{"path":"kmeans.html","id":"visualise-the-data","chapter":"2 K-means clustering","heading":"2.5 Visualise the data","text":"First , let’s look data. always good idea get sense data.tidyverse\nFirst, load inspect data:Next, plot data:base R\nFirst, load inspect data:Next, plot data:Python\npalmerpenguins package available Python, ’ve provided data separately. load data Python following:Next, plot data:different types penguins, different islands. Bill flipper measurements taken, penguins’ weight plus sex recorded.example, also look bill depth versus bill length.can already see data appear cluster quite closely species. great example illustrate K-means clustering (’d almost think chose example purpose!)","code":"\n# attach the data\ndata(package = 'palmerpenguins')\n\n# inspect the data\npenguins## # A tibble: 344 × 8\n##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n##    <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n##  1 Adelie  Torgersen           39.1          18.7               181        3750\n##  2 Adelie  Torgersen           39.5          17.4               186        3800\n##  3 Adelie  Torgersen           40.3          18                 195        3250\n##  4 Adelie  Torgersen           NA            NA                  NA          NA\n##  5 Adelie  Torgersen           36.7          19.3               193        3450\n##  6 Adelie  Torgersen           39.3          20.6               190        3650\n##  7 Adelie  Torgersen           38.9          17.8               181        3625\n##  8 Adelie  Torgersen           39.2          19.6               195        4675\n##  9 Adelie  Torgersen           34.1          18.1               193        3475\n## 10 Adelie  Torgersen           42            20.2               190        4250\n## # … with 334 more rows, and 2 more variables: sex <fct>, year <int>\nggplot(penguins, aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     colour = species)) +\n  geom_point()\n# attach the data\ndata(package = 'palmerpenguins')\n\n# inspect the data\nhead(penguins)## # A tibble: 6 × 8\n##   species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  \n##   <fct>   <fct>           <dbl>         <dbl>            <int>       <int> <fct>\n## 1 Adelie  Torge…           39.1          18.7              181        3750 male \n## 2 Adelie  Torge…           39.5          17.4              186        3800 fema…\n## 3 Adelie  Torge…           40.3          18                195        3250 fema…\n## 4 Adelie  Torge…           NA            NA                 NA          NA <NA> \n## 5 Adelie  Torge…           36.7          19.3              193        3450 fema…\n## 6 Adelie  Torge…           39.3          20.6              190        3650 male \n## # … with 1 more variable: year <int>\nplot(penguins$bill_depth_mm,      # scatter plot\n     penguins$bill_length_mm,\n     pch = 20,\n     col = penguins$species)      # colour by species\n\nlegend(\"bottomright\",             # legend\n       legend = levels(penguins$species),\n       pch = 20,\n       col = factor(levels(penguins$species)))# load the data\npenguins = pd.read_csv('data/penguins.csv')\n\n# inspect the data\npenguins.head()##    species     island  bill_length_mm  ...  body_mass_g      sex    year\n##   <object>   <object>       <float64>  ...    <float64> <object> <int64>\n## 0   Adelie  Torgersen            39.1  ...       3750.0     male    2007\n## 1   Adelie  Torgersen            39.5  ...       3800.0   female    2007\n## 2   Adelie  Torgersen            40.3  ...       3250.0   female    2007\n## 3   Adelie  Torgersen             NaN  ...          NaN      NaN    2007\n## 4   Adelie  Torgersen            36.7            3450.0   female    2007\n## \n## [5 rows x 8 columns](\n  ggplot(penguins,\n    aes(x = 'bill_depth_mm',\n        y = 'bill_length_mm',\n        colour = 'species'))\n  + geom_point()\n)"},{"path":"kmeans.html","id":"exercise---flipper-and-bill-length","chapter":"2 K-means clustering","heading":"2.6 Exercise - Flipper and bill length","text":"Exercise 2.1  exercise ’d like create scatter plot flipper length bill length. ’ll using colour separate female/male data. Lastly, ’re creating individual panels island.code given like replace <FIXME> parts required code.think many clusters try divide data .tidyversebase R\nThings bit convoluted using base R, compared tidyverse. ’s faceting equivalent can implement directly.split data island create loop plot island. ’m loathe go , ’ll just use Biscoe island example ’m sure ’re able adapt things accordingly Dream Torgersen islands!Pythontidyversebase RPython","code":"penguins %>% \n  drop_na() %>%                     \n  ggplot(aes(x = <FIXME>,\n             y = bill_length_mm,\n             colour = <FIXME>)) +\n  geom_<FIXME>() +\n  facet_wrap(facets = vars(<FIXME>))biscoe <- \n  penguins[penguins$island == '<FIXME>', ]\n\nplot(biscoe$<FIXME>,\n     biscoe$bill_length_mm,\n     pch = 20,\n     col = biscoe$<FIXME>)\n\nlegend(\"bottomright\",\n       legend = levels(biscoe$sex),\n       pch = 20,\n       col = factor(levels(biscoe$sex)))\n\ntitle(main = \"Biscoe\")(\n  penguins >> \\\n  drop_na() >> \\\n  ggplot(aes(x = '<FIXME>',\n             y = 'bill_length_mm',\n             colour = '<FIXME>'))\n         + geom_<FIXME>()\n         + facet_wrap('<FIXME>')\n)\npenguins %>% \n  drop_na() %>%                     \n  ggplot(aes(x = flipper_length_mm,\n             y = bill_length_mm,\n             colour = sex)) +\n  geom_point() +\n  facet_wrap(facets = vars(island))\nbiscoe <- \n  penguins[penguins$island == 'Biscoe', ]\n\nplot(biscoe$flipper_length_mm,\n     biscoe$bill_length_mm,\n     pch = 20,\n     col = biscoe$sex)\n\nlegend(\"bottomright\",\n       legend = levels(biscoe$sex),\n       pch = 20,\n       col = factor(levels(biscoe$sex)))\n\ntitle(main = 'Biscoe')(\n  penguins >> \\\n  drop_na() >> \\\n  ggplot(aes(x = 'flipper_length_mm',\n             y = 'bill_length_mm',\n             colour = 'sex'))\n         + geom_point()\n         + facet_wrap('island')\n)"},{"path":"kmeans.html","id":"clustering","chapter":"2 K-means clustering","heading":"2.7 Clustering","text":"Next, ’ll actual clustering.tidyverse\nclustering, ’ll using kmeans() function. function requires numeric data input. also scaling data. Although required case, variables unit (millimetres) good practice. scenarios different units compared, affect clustering.Note output list vectors, differing lengths. ’s contain different types information:cluster contains information pointcenters, withinss, size contain information clustertotss, tot.withinss, betweenss, iter contain information full clusteringbase R\nclustering, ’ll using kmeans() function. function requires numeric data input. also scaling data. Although required case, variables unit (millimetres) good practice. scenarios different units compared, affect clustering.Note output list vectors, differing lengths. ’s contain different types information:cluster contains information pointcenters, withinss, size contain information clustertotss, tot.withinss, betweenss, iter contain information full clusteringPython\nclustering, ’ll using KMeans() function. function requires numeric data input. also scaling data. Although required case, variables unit (millimetres) good practice. scenarios different units compared, affect clustering.","code":"\npoints <-\n  penguins %>% \n  select(bill_depth_mm,          # select data\n         bill_length_mm) %>% \n  drop_na() %>%                  # remove missing values\n  scale() %>%                    # scale the data\n  as_tibble() %>% \n  rename(bill_depth_scaled = bill_depth_mm,\n         bill_length_scaled = bill_length_mm)\n\n\nkclust <-\n  kmeans(points,                 # perform k-means clustering\n         centers = 3)            # using 3 centers\n\nsummary(kclust)                  # summarise output##              Length Class  Mode   \n## cluster      342    -none- numeric\n## centers        6    -none- numeric\n## totss          1    -none- numeric\n## withinss       3    -none- numeric\n## tot.withinss   1    -none- numeric\n## betweenss      1    -none- numeric\n## size           3    -none- numeric\n## iter           1    -none- numeric\n## ifault         1    -none- numeric\npoints_r <-\n  data.frame(\n    penguins$bill_depth_mm,      # get the numeric data\n    penguins$bill_length_mm) |>  # use base R pipe!\n  na.omit() |>                   # remove missing data\n  scale()                        # scale the data\n\n# and rename the columns to avoid confusion\nnames(points_r)[1] <- 'bill_depth_scaled'\nnames(points_r)[2] <- 'bill_length_scaled'\n\nkclusts_r <-\n  kmeans(points_r,               # perform k-means clustering\n         centers = 3)            # using 3 centers\n\nsummary(kclusts_r)                # summarise output##              Length Class  Mode   \n## cluster      342    -none- numeric\n## centers        6    -none- numeric\n## totss          1    -none- numeric\n## withinss       3    -none- numeric\n## tot.withinss   1    -none- numeric\n## betweenss      1    -none- numeric\n## size           3    -none- numeric\n## iter           1    -none- numeric\n## ifault         1    -none- numericscaler = StandardScaler()\n\npoints_py = \\\npenguins >> \\\n  drop_na() >> \\\n  select('bill_depth_mm', 'bill_length_mm')\n\npoints_py = scaler.fit_transform(points_py)\n\nkmeans = KMeans(\ninit = 'random',\nn_clusters = 3,\nn_init = 10,\nmax_iter = 300,\nrandom_state = 42\n)\n\nkmeans.fit(points_py)## KMeans(init='random', n_clusters=3, random_state=42)"},{"path":"kmeans.html","id":"visualise-clusters","chapter":"2 K-means clustering","heading":"2.8 Visualise clusters","text":"can visualise clusters calculated .tidyverse\nperformed clustering, centers calculated. values give (x, y) coordinates centroids.initial centroids get randomly placed data. , combined iterative nature process, means values see going slightly different values . ’s normal!Next, want visualise data points belong cluster. can follows:base R\nperformed clustering, centers calculated. values give (x, y) coordinates centroids.initial centroids get randomly placed data. , combined iterative nature process, means values see going slightly different values . ’s normal!Next, want visualise data points belong cluster. can follows:Python\nperformed clustering, centers calculated. values give (x, y) coordinates centroids.initial centroids get randomly placed data. , combined iterative nature process, means values see going slightly different values . ’s normal!Next, want visualise data points belong cluster. can follows:","code":"\ntidy_clust <- tidy(kclust) # get centroid coordinates\n\ntidy_clust## # A tibble: 3 × 5\n##   bill_depth_scaled bill_length_scaled  size withinss cluster\n##               <dbl>              <dbl> <int>    <dbl> <fct>  \n## 1             0.799              1.10     64     39.0 1      \n## 2             0.560             -0.943   153     88.0 2      \n## 3            -1.09               0.590   125     59.4 3\nkclust %>%                              # take clustering data\n  augment(points) %>%                   # combine with original data\n  ggplot(aes(x = bill_depth_scaled,     # plot the scaled data\n             y = bill_length_scaled)) +\n  geom_point(aes(colour = .cluster)) +  # colour by classification\n  geom_point(data = tidy_clust,\n             size = 7, shape = 'x')     # add the cluster centers\nkclusts_r$centers  # get centroid coordinates##   penguins.bill_depth_mm penguins.bill_length_mm\n## 1             -1.0937700               0.5903143\n## 2              0.5595723              -0.9431819\n## 3              0.7985421               1.1018368\nplot(points_r,                # plot scaled data\n     col = kclusts_r$cluster,  # colour by cluster\n     pch = 20)\n\npoints(kclusts_r$centers,      # add cluster centers\n       pch = 4,\n       lwd = 3)# calculate the cluster centers\nkclusts_py = kmeans.cluster_centers_\n\n# convert to tibble and rename columns\n# use base0_ = True because indices are 0-based\nkclusts_py = \\\ntibble(kclusts_py) >> \\\nrename(bill_depth_scaled = 0,\n       bill_length_scaled = 1, base0_=True)\n\n# and show the coordinates\nkclusts_py##    bill_depth_scaled  bill_length_scaled\n##            <float64>           <float64>\n## 0          -1.098055            0.586444\n## 1           0.795036            1.088684\n## 2           0.553935           -0.950240# convert NumPy arrays to data frame\n# use base0_ = True because indices are 0-based\npoints_py = \\\ntibble(points_py) >> \\\nrename(bill_depth_scaled = 0,\n       bill_length_scaled = 1, base0_ = True)\n\n# merge with original data\n# add predicted clusters\npenguins_clusters = \\\npenguins >> \\\n  drop_na() >> \\\n  bind_cols(points_py) >> \\\n  mutate(cluster = factor(kmeans.fit_predict(points_py)))(\nggplot(penguins_clusters,\n       aes(x = 'bill_depth_mm',\n           y = 'bill_length_mm',\n           colour = 'cluster'))\n         + geom_point()\n)"},{"path":"kmeans.html","id":"optimising-cluster-number","chapter":"2 K-means clustering","heading":"2.9 Optimising cluster number","text":"example set number clusters 3. made sense, data already visually separated roughly three groups - one species.However, might cluster number choose lot less obvious. case helpful explore clustering data range clusters.short, determine values k want explore loop values, repeating workflow looked previously.tidyverse\nReiterating range k values reasonably straightforward using tidyverse. Although write function loop k values, tidyverse series map() functions can . information .short, map() function spits list contains output. data, can create table contains lists information need.calculate following:kclust column contains list kmeans() output, value kthe tidied column contains information per-cluster basisthe glanced column contains single-row summary k - ’ll use tot.withinss values little bit later onthe augmented column contains original data, augmented classification calculated kmeans() functionLists can sometimes bit tricky get head around, ’s worthwhile exploring output. RStudio particularly useful , since can just left-click object Environment panel look.way see lists context containers. one huge table kclusts contains information need. ‘cell’ table container relevant data. kclust column list kmeans objects (output kmeans() k values), whereas columns lists tibbles (tidy(), glance() augment() functions output tibble information value k).us use data lists, makes sense extract column--column basis. ’re ignoring kclust column, don’t need actual kmeans() output .extract data lists use unnest() function.Next, can visualise data. ’ll start plotting scaled data colouring data points based final cluster assigned kmeans() function.(augmented) data assignments. look structure table.facet data k, get single panel value k.also add calculated cluster centres, stored clusters.Looking plot shows already knew (things easy time!): three clusters pretty good choice data. Remember ’re looking clusters distinct, .e. separated one another. example, using k = 4 gives four nice groups, two directly adjacent, suggesting equally well single cluster.base R","code":"\nkclusts <- \n  tibble(k = 1:6) %>%                         # check for k = 1 to 6\n  mutate(\n    kclust = map(k, ~kmeans(points, .x)),     # perform clustering for each k\n    tidied = map(kclust, tidy),               # summary at per-cluster level\n    glanced = map(kclust, glance),            # get single-row summary\n    augmented = map(kclust, augment, points)  # add classification to data set\n  )\n\nkclusts## # A tibble: 6 × 5\n##       k kclust   tidied           glanced          augmented         \n##   <int> <list>   <list>           <list>           <list>            \n## 1     1 <kmeans> <tibble [1 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n## 2     2 <kmeans> <tibble [2 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n## 3     3 <kmeans> <tibble [3 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n## 4     4 <kmeans> <tibble [4 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n## 5     5 <kmeans> <tibble [5 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\n## 6     6 <kmeans> <tibble [6 × 5]> <tibble [1 × 4]> <tibble [342 × 3]>\nclusters <- \n  kclusts %>%\n  unnest(cols = c(tidied))\n\nassignments <- \n  kclusts %>% \n  unnest(cols = c(augmented))\n\nclusterings <- \n  kclusts %>%\n  unnest(cols = c(glanced))\nggplot(assignments,\n       aes(x = bill_depth_scaled,     # plot data\n           y = bill_length_scaled)) +  \n  geom_point(aes(color = .cluster),   # colour by cluster\n             alpha = 0.8) +           # add transparency\n  facet_wrap(~ k) +                   # facet for each k\n  geom_point(data = clusters,         # add centers\n             size = 7,\n             shape = \"x\")\n#baseR explore"},{"path":"kmeans.html","id":"elbow-plot","chapter":"2 K-means clustering","heading":"2.9.1 Elbow plot","text":"Visualising data like can helpful time can also bit subjective (hoorah!). find another subjective way interpreting clusters (remember, statistics isn’t YES/magic mushroom comfortable wandering around murky grey areas statistics now), can plot total within-cluster variation value k.Intuitively, keep adding clusters total amount variation can explained clusters increase. extreme case data point cluster can explain variation data.course sensible approach - hence us balancing number clusters much variation can capture.practical approach creating “elbow” plot cumulative amount variation explained plotted number clusters.tidyverse\noutput kmeans() function includes tot.withinss - total within-cluster sum squares.base RWe can see total within-cluster sum squares decreases number clusters increases. can also see k = 3 onwards slope line becomes much shallower. “elbow” bending point useful gauge find optimum number clusters.exploration can see three clusters optimal scenario.","code":"\nggplot(clusterings,\n       aes(x = k,                # for each k plot...\n           y = tot.withinss)) +  # total within variance\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(\n    breaks = seq(1, 6, 1))       # set the x-axis breaks"},{"path":"kmeans.html","id":"exercise","chapter":"2 K-means clustering","heading":"2.10 Exercise","text":"Exercise 2.2  Practice running clustering workflow using finches dataset. Try following:Read dataExplore visualise dataPerform clustering k = 2Find using k = 2 reasonable choiceTry draw conclusionstidyverseLet’s look data.Next, perform clustering.looks like two clusters reasonable choice. let’s explore bit .Extract relevant data.Visualise result.Create elbow plot closer look.initial clustering done using two clusters, basically capturing two different finch species.Redoing analysis different numbers clusters seems reasonably support decision. elbow plot suggests k = 3 terrible idea either.example used data collected two different time points: 1975 2012.analysis ’ve kept data together. However, original premises data see indication evolution going species finches. Think approach question!base R\nadded.","code":"\nfinches <- read_csv(\"data/finch_beaks.csv\")\nhead(finches)## # A tibble: 6 × 5\n##    band species beak_length_mm beak_depth_mm  year\n##   <dbl> <chr>            <dbl>         <dbl> <dbl>\n## 1     2 fortis             9.4           8    1975\n## 2     9 fortis             9.2           8.3  1975\n## 3    12 fortis             9.5           7.5  1975\n## 4    15 fortis             9.5           8    1975\n## 5   305 fortis            11.5           9.9  1975\n## 6   307 fortis            11.1           8.6  1975\nggplot(finches, aes(x = beak_depth_mm,\n                     y = beak_length_mm,\n                     colour = species)) +\n  geom_point()\npoints <-\n  finches %>% \n  select(beak_depth_mm,         # select data\n         beak_length_mm) %>% \n  drop_na()                     # remove missing values\n\nkclust <-\n  kmeans(points,                 # perform k-means clustering\n         centers = 2)            # using 2 centers\n\nsummary(kclust)                  # summarise output##              Length Class  Mode   \n## cluster      651    -none- numeric\n## centers        4    -none- numeric\n## totss          1    -none- numeric\n## withinss       2    -none- numeric\n## tot.withinss   1    -none- numeric\n## betweenss      1    -none- numeric\n## size           2    -none- numeric\n## iter           1    -none- numeric\n## ifault         1    -none- numeric\ntidy_clust <- tidy(kclust) # get centroid coordinates\n\ntidy_clust## # A tibble: 2 × 5\n##   beak_depth_mm beak_length_mm  size withinss cluster\n##           <dbl>          <dbl> <int>    <dbl> <fct>  \n## 1          8.98           10.5   431     442. 1      \n## 2          9.16           13.7   220     237. 2\nkclust %>%                              # take clustering data\n  augment(points) %>%                   # combine with original data\n  ggplot(aes(x = beak_depth_mm,         # plot the original data\n             y = beak_length_mm)) +\n  geom_point(aes(colour = .cluster)) +  # colour by classification\n  geom_point(data = tidy_clust,\n             size = 7, shape = 'x')     # add the cluster centers\nkclusts <- \n  tibble(k = 1:6) %>%                         # check for k = 1 to 6\n  mutate(\n    kclust = map(k, ~kmeans(points, .x)),     # perform clustering for each k\n    tidied = map(kclust, tidy),               # summary at per-cluster level\n    glanced = map(kclust, glance),            # get single-row summary\n    augmented = map(kclust, augment, points)  # add classification to data set\n  )\nclusters <- \n  kclusts %>%\n  unnest(cols = c(tidied))\n\nassignments <- \n  kclusts %>% \n  unnest(cols = c(augmented))\n\nclusterings <- \n  kclusts %>%\n  unnest(cols = c(glanced))\nggplot(assignments,\n       aes(x = beak_depth_mm,        # plot data\n           y = beak_length_mm)) +  \n  geom_point(aes(color = .cluster),  # colour by cluster\n             alpha = 0.8) +          # add transparency\n  facet_wrap(~ k) +                  # facet for each k\n  geom_point(data = clusters,        # add centers\n             size = 7,\n             shape = 'x')\nggplot(clusterings,\n       aes(x = k,                # for each k plot...\n           y = tot.withinss)) +  # total within variance\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(\n    breaks = seq(1, 6, 1))       # set the x-axis breaks"},{"path":"kmeans.html","id":"load-the-data","chapter":"2 K-means clustering","heading":"Load the data","text":"","code":"\nfinches <- read_csv(\"data/finch_beaks.csv\")\nhead(finches)## # A tibble: 6 × 5\n##    band species beak_length_mm beak_depth_mm  year\n##   <dbl> <chr>            <dbl>         <dbl> <dbl>\n## 1     2 fortis             9.4           8    1975\n## 2     9 fortis             9.2           8.3  1975\n## 3    12 fortis             9.5           7.5  1975\n## 4    15 fortis             9.5           8    1975\n## 5   305 fortis            11.5           9.9  1975\n## 6   307 fortis            11.1           8.6  1975"},{"path":"kmeans.html","id":"visualise-the-data-1","chapter":"2 K-means clustering","heading":"Visualise the data","text":"Let’s look data.","code":"\nggplot(finches, aes(x = beak_depth_mm,\n                     y = beak_length_mm,\n                     colour = species)) +\n  geom_point()"},{"path":"kmeans.html","id":"clustering-1","chapter":"2 K-means clustering","heading":"Clustering","text":"Next, perform clustering.","code":"\npoints <-\n  finches %>% \n  select(beak_depth_mm,         # select data\n         beak_length_mm) %>% \n  drop_na()                     # remove missing values\n\nkclust <-\n  kmeans(points,                 # perform k-means clustering\n         centers = 2)            # using 2 centers\n\nsummary(kclust)                  # summarise output##              Length Class  Mode   \n## cluster      651    -none- numeric\n## centers        4    -none- numeric\n## totss          1    -none- numeric\n## withinss       2    -none- numeric\n## tot.withinss   1    -none- numeric\n## betweenss      1    -none- numeric\n## size           2    -none- numeric\n## iter           1    -none- numeric\n## ifault         1    -none- numeric\ntidy_clust <- tidy(kclust) # get centroid coordinates\n\ntidy_clust## # A tibble: 2 × 5\n##   beak_depth_mm beak_length_mm  size withinss cluster\n##           <dbl>          <dbl> <int>    <dbl> <fct>  \n## 1          8.98           10.5   431     442. 1      \n## 2          9.16           13.7   220     237. 2"},{"path":"kmeans.html","id":"visualise-the-clusters","chapter":"2 K-means clustering","heading":"Visualise the clusters","text":"","code":"\nkclust %>%                              # take clustering data\n  augment(points) %>%                   # combine with original data\n  ggplot(aes(x = beak_depth_mm,         # plot the original data\n             y = beak_length_mm)) +\n  geom_point(aes(colour = .cluster)) +  # colour by classification\n  geom_point(data = tidy_clust,\n             size = 7, shape = 'x')     # add the cluster centers"},{"path":"kmeans.html","id":"optimise-clusters","chapter":"2 K-means clustering","heading":"Optimise clusters","text":"looks like two clusters reasonable choice. let’s explore bit .Extract relevant data.Visualise result.Create elbow plot closer look.","code":"\nkclusts <- \n  tibble(k = 1:6) %>%                         # check for k = 1 to 6\n  mutate(\n    kclust = map(k, ~kmeans(points, .x)),     # perform clustering for each k\n    tidied = map(kclust, tidy),               # summary at per-cluster level\n    glanced = map(kclust, glance),            # get single-row summary\n    augmented = map(kclust, augment, points)  # add classification to data set\n  )\nclusters <- \n  kclusts %>%\n  unnest(cols = c(tidied))\n\nassignments <- \n  kclusts %>% \n  unnest(cols = c(augmented))\n\nclusterings <- \n  kclusts %>%\n  unnest(cols = c(glanced))\nggplot(assignments,\n       aes(x = beak_depth_mm,        # plot data\n           y = beak_length_mm)) +  \n  geom_point(aes(color = .cluster),  # colour by cluster\n             alpha = 0.8) +          # add transparency\n  facet_wrap(~ k) +                  # facet for each k\n  geom_point(data = clusters,        # add centers\n             size = 7,\n             shape = 'x')\nggplot(clusterings,\n       aes(x = k,                # for each k plot...\n           y = tot.withinss)) +  # total within variance\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(\n    breaks = seq(1, 6, 1))       # set the x-axis breaks"},{"path":"kmeans.html","id":"conclusions","chapter":"2 K-means clustering","heading":"Conclusions","text":"initial clustering done using two clusters, basically capturing two different finch species.Redoing analysis different numbers clusters seems reasonably support decision. elbow plot suggests k = 3 terrible idea either.","code":""},{"path":"kmeans.html","id":"food-for-thought","chapter":"2 K-means clustering","heading":"Food for thought","text":"example used data collected two different time points: 1975 2012.analysis ’ve kept data together. However, original premises data see indication evolution going species finches. Think approach question!","code":""},{"path":"kmeans.html","id":"key-points","chapter":"2 K-means clustering","heading":"2.11 Key points","text":"k-means clustering partitions data clustersthe k defines number clusterscluster centers centroids get assigned randomlyeach data point gets assigned closest centroidthe centroid new clusters gets calculated process assignment recalculation repeats cluster longer changethe optimal number clusters can determined ‘elbow’ plot","code":""}]
